[{"content":"什么是 gRPC？\nA high performance, open source universal RPC framework.\ngRCP 是一个高性能、开源的、通用的 RPC 框架：\n从上图可以看出：\ngRPC 分为服务端（Server）和客户端（Stub/Client）两个部分 服务端和客户端之间通过 Protocal Buffers 协议进行通讯 服务端和客户端支持跨语言调用 gRPC 支持众多主流编程语言：\n下面通过一个 Hello world demo 来逐步了解 gRPC。\nProtocal Buffers Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.\n简单来说 Protocal Buffers 是一套用于序列化和反序列化数据的机制，与平台无关，与语言无关，gRPC 默认使用这套机制。举个例子 ./proto/greeter.proto：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 syntax = \u0026#34;proto3\u0026#34;; option go_package = \u0026#34;./greeter\u0026#34;; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } syntax = \u0026quot;proto3\u0026quot;; 是固定写法，显式声明使用 Protocal Buffers 的 version 3 版本。 option go_package = \u0026quot;./greeter\u0026quot;; 是定义生成 Go 语言的配置，这里的配置会在当前目录下生成一个 greeter 目录，里面的报名也是 greeter。其他语言写法大同小异。 service 是 Protocal Buffers 用来定义一个微服务的关键字，service 里面可以定义各种微服务需要提供的方法，最主要的就是要声明入参（这里是 HelloRequest）和出参（这里是 HelloReply）。 message 是 Protocal Buffers 用来定义一个结构体的关键字，对应的 Go 语言的 struct 关键字。 Protocal Buffers 有自己的基本数据类型，并且能契合各种编程语言：\nProtocal Buffers 使用 .proto 作为文件后缀，定义好相关内容后，就可以使用配套的工具生成对应编程语言所需要的代码文件，这里以 Go 语言为例，在 ./proto 目录下执行：\n1 protoc --go_out=./ *.proto 这个命令主要是根据 message 关键字定义的内容生成 Go 语言对应的结构体及一些相关的函数，会生成一个名为 ./proto/greeter/greeter.pb.go 文件，这个文件可以直接在 Go 语言代码中引用，具体内容就不展开了。\n1 protoc --go-grpc_out=./ *.proto 这个命令会生成 gRPC 所需的一些接口和方法，会生成一个名为 ./proto/greeter/greeter_grpc.pb.go 文件，这个文件里包含很多内容，这里挑几个重要的解释一下：\n1 2 3 4 5 // The greeting service definition. type GreeterClient interface { // Sends a greeting SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) } 这个 interface 是根据 Protocal Buffer 中 service 关键字定义的内容来生成的，定义一个 gRPC 接口，包含相关的函数。\n1 2 3 4 5 6 7 8 9 10 func RegisterGreeterServer(s grpc.ServiceRegistrar, srv GreeterServer) { // If the following call pancis, it indicates UnimplementedGreeterServer was // embedded by pointer and is nil. This will cause panics if an // unimplemented method is ever invoked, so we test this at initialization // time to prevent it from happening at runtime later due to I/O. if t, ok := srv.(interface{ testEmbeddedByValue() }); ok { t.testEmbeddedByValue() } s.RegisterService(\u0026amp;Greeter_ServiceDesc, srv) } 这一段是用于 gRPC 服务端注册服务时使用的。\n1 2 3 func NewGreeterClient(cc grpc.ClientConnInterface) GreeterClient { return \u0026amp;greeterClient{cc} } 这一段是用于 gRPC 客户端跟服务端建立连接时使用的。\n通过 Protocal Buffers 配套工具生成的代码文件是服务端和客户端通用的，一份 .proto 文件可以生成多份不同编程语言的代码。上述生成 Go 语言代码的例子用到了 3 个工具：\n1 2 3 brew install protobuf brew install protoc-gen-go brew install protoc-gen-go-grpc protobuf 是主程序，在命令行通过 protoc 使用。 protoc-gen-go 对应 --go_out 参数，用于生成 greeter.pb.go 文件。 protoc-gen-go-grpc 对应 --go-grpc_out 参数，用于生成 greeter_grpc.pb.go 文件。 服务端（Server） 服务端的构建分为以下几个步骤：\n新建一个结构体，将 Protocal Buffers 中定义的服务的方法实现并绑定到这个结构体。 新建一个 gRPC 服务。 将上述新建的结构体注册到新建的 gRPC 服务中。 新建一个监听器，用于将 gRPC 服务暴露到某个端口。 启动 gRPC 服务。 完整的代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type server struct { greeter.UnimplementedGreeterServer } func (s *server) SayHello(_ context.Context, in *greeter.HelloRequest) (*greeter.HelloReply, error) { return \u0026amp;greeter.HelloReply{Message: \u0026#34;Hello \u0026#34; + in.Name}, nil } func main() { grpcServer := grpc.NewServer() greeter.RegisterGreeterServer(grpcServer, \u0026amp;server{}) listener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:8080\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } defer listener.Close() if err := grpcServer.Serve(listener); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } } 客户端（Stub/Client） 客户端调用服务端分为以下几个步骤：\n新建一个 gRPC 客户端连接。 使用这个 gRPC 连接新建一个 Protocal Buffers 生成的客户端。 使用新建的客户端直接调用服务端的方法。 完整的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { clientConn, err := grpc.NewClient(\u0026#34;localhost:8080\u0026#34;, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { fmt.Println(\u0026#34;Error connecting to server\u0026#34;) return } greeterClient := greeter.NewGreeterClient(clientConn) res, err := greeterClient.SayHello(context.Background(), \u0026amp;greeter.HelloRequest{Name: \u0026#34;World\u0026#34;}) if err != nil { fmt.Println(\u0026#34;Error calling SayHello\u0026#34;) return } fmt.Println(res.Message) } 这里的 grpc.WithTransportCredentials(insecure.NewCredentials()) 参数是指客户端不实用任何安全机制跟服务端通讯，是明文传输，这里是本地 demo 调试，不用在意。生产环境通常需要配置 TLS，使用 credentials.NewTLS()。\n参考资料 https://grpc.io/docs/what-is-grpc/introduction/ https://protobuf.dev/programming-guides/proto3/ ","date":"2025-03-02T07:14:00+08:00","permalink":"https://www.blackmatch.cn/p/grpc-%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98/","title":"gRPC 学习备忘"},{"content":"概述 学习 Go 的同学，肯定或多或少都对 Go 的 GMP 调度模型有所了解，网上也很多好的学习资料，这篇文章不会涉及太多 GMP 相关的理论知识。 GMP 模型能让 Go 程序尽可能压榨 CPU 的性能，从而提高并发处理能力。本文主要是从一个场景通过 debug 的方式去一窥 GMP 的工作方式，通过可视化的方式加深对 GMP 的理解。为了方便阅读先简单介绍一下 GMP：\nG：Goroutine ，Go 语言调度器中待执行的任务。 M：Machine，操作系统线程，用来执行 G 的，调度器最多可以创建 10000 个线程，但最多只会有 GOMAXPROCS 个活跃线程能够正常运行。 P：Processer，处理器，是 G 和 M 的中间层，它能提供线程需要的上下文环境，也会负责调度线程上的等待队列。 场景：开 10000 个 Goroutine 并发读取 10000 个文件 在没有开始学习 Go 语言之前，我就听说 Go 的协程很厉害，轻量级，开几万个协程都可以。然后我就想试一试开 10000 个 Goroutine 看看是什么情况：\n先在本地创建 10000 个 txt 文件，每个文件的大小为 1M： 开 10000 个 Goroutine 并发读取： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func main() { var wg sync.WaitGroup wg.Add(10000) for i := range 10000 { go func() { defer wg.Done() _, err := os.ReadFile(fmt.Sprintf(\u0026#34;%d.txt\u0026#34;, i)) if err != nil { fmt.Println(err) } }() } wg.Wait() fmt.Println(\u0026#34;done\u0026#34;) } 代码写好开始执行：\n没想到竟然炸了，但好像又在意料之中。查了下这种情况是：操作系统对每个进程可以创建的线程数量有一定的限制。当 Go 程序尝试创建大量的 goroutine 时，每个 goroutine 最终可能需要一个操作系统线程来执行，一旦超过系统的线程数量限制，就会出现此错误。简而言之就是并发的 goroutine 太多了，我把并发量减少到 100，同时通过 trace 包记录下运行时的数据方便后续分析，完整的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func main() { f, err := os.Create(\u0026#34;trace.out\u0026#34;) if err != nil { panic(err) } defer f.Close() trace.Start(f) defer trace.Stop() batchSize := 100 for start := 0; start \u0026lt; 10000; start += batchSize { var wg sync.WaitGroup end := start + batchSize if end \u0026gt; 10000 { end = 10000 } for i := start; i \u0026lt; end; i++ { filename := fmt.Sprintf(\u0026#34;%d.txt\u0026#34;, i) wg.Add(1) go readFile(filename, \u0026amp;wg) } wg.Wait() } fmt.Println(\u0026#34;Done\u0026#34;) } func readFile(fileName string, wg *sync.WaitGroup) { defer wg.Done() _, err := os.ReadFile(fileName) if err != nil { fmt.Println(err) } } 这次能成功运行了，会在当前目录下生成一个 trace.out 文件，我们执行：\n1 go tool trace trace.out 然后根据提示打开浏览器就可以看到如下内容：\n点击彩色部分可以在底下看到相关的信息，例如上图中是某一时刻 goroutine 的情况，可以看到当前 G0 处于 GCWaiting 状态，正在执行的 G 的 id 是 85，其他的暂时不展开了。通过 trace 包我们可以通过可视化的方式对 GMP 有了大概的感知，接下来我们继续分析 GMP 的调度。 我们把 trace 相关的代码去掉，把 G 的并发两加到 1000，然后通过 DEBUG 工具来分析：\n1 GODEBUG=schedtrace=500 ./main 解释一下这个命令和输出的内容，这个命令是通过 debug 的方式启动 Go 程序，其中的 schedtrace 是指定 debug 的内容为调度器相关的数据，500 表示的是每隔 500 毫秒打印一次信息。关于输出的内容：\nxxms 表示程序启动到打印当前信息过去了多长时间。 gomaxprocs 表示最多会有多少个 P 来处理 G，默认为CPU 的核心数，可以通过环境变量 GOMAXPROCS 和 runtime.GOMAXPROCS()这个函数来调整。 idleprocs 表示处于空闲状态的 P 数量。 threads 表示正在运行的线程数量。 spinningthreads 表示处于自旋状态的线程的数量。在 Go 运行时的调度系统中，自旋是一种线程的状态，处于自旋状态的线程不会进入休眠，而是持续地检查是否有可执行的任务。 needspinning 是一个用于判断是否需要启动新的自旋线程的条件。Go 运行时的调度器会根据当前系统的状态和任务负载来决定是否需要创建新的自旋线程。 idlethreads 表示处于空闲状态的线程数量。 runqueue 表示全局队列中正在等待被执行的 G 的数量。 [0 0 0 0 0 0 0 0] 表示每个 P 的本地队列中等待被执行的 G 的数量。数组的长度为 GOMAXPROCS 。 从上图可以观察到一个比较奇怪的现象：为什么数组里的元素的值都是 0，也就是说每个 P 的本地队列都是空的。稍加思索后，我想到可能是因为我设置了每隔 500 毫秒才打印一次信息，可能程序执行太快，500 毫秒后所有的 G 都已经执行完成或者正在执行中，所以本地队列里都是空的。我调成 100 毫秒后结果如下：\n从上图中可以看出：\nidleprocs 的值在不断变化，但最大不会超过 gomaxprocs 的值。 threads 稳定在 90 多，说明同时有 90 多个线程在工作。 在全局队列几乎为 0 的情况下，各个 P 的本地队列数量之和是在逐步减少的，但是某个 P 的队列的数量会突然增多。 随后我又将 G 的并发调成 2000 进行调试，得到如下结果：\n结合 GMP 的理论知识，我们可以得到以下结论：\nGo 会开启多线程去并发执行 goroutine，线程的数量会根据 goroutine 的数量进行调整。 GMP 通过 hand off 机制，当正在执行的 G 发生阻塞时，会把 G 与当前的 P 断开，将 P 及其本地队列重新绑定到一个休眠的 M 上或者新建一个 M 来绑定。可以从调试的结果中看出在执行到后期 idleprocs 的值等于 gomaxprocs 的值了，这是因为 G 执行的都是阻塞任务（读取文件），每个 G 经过 P 的短暂处理后就会跟 P 断开，所以程序执行到后面会有很多空闲的 P，G 都已经进入系统调用状态，最后被放入全局队列后最终被执行完成。 GMP 通过 work stealing 机制，当一个 P 的本地队列和全局队列都空了的时候，它就从其他 P 的队列中偷一些 G 过来放到自己的本地队列中。图中数组倒数第二个元素最后一个大于 0 的值为 54，这个值大于之前出现过的所有的值，就说明对应的 P 是从其他 P 的本地队列中偷了一部分 G 过来了。 综上，Go 的 GMP 模型使其能够最大限度地压榨 CPU 的性能，从而能够支撑更高的并发，提升程序的性能。\n参考资料 刘丹冰 B 站视频 https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-goroutine/#65-%E8%B0%83%E5%BA%A6%E5%99%A8 ","date":"2025-02-28T15:41:00+08:00","permalink":"https://www.blackmatch.cn/p/%E4%BB%8E%E5%9C%BA%E6%99%AF%E5%8E%BB%E7%90%86%E8%A7%A3-go-%E7%9A%84-gmp-%E6%A8%A1%E5%9E%8B%E9%82%A3%E6%88%91%E5%BC%80-10000-%E4%B8%AA-goroutine-%E4%BC%9A%E6%80%8E%E6%A0%B7/","title":"从场景去理解 Go 的 GMP 模型：那我开 10000 个 Goroutine 会怎样？"},{"content":"DeepSeek 为什么这么火 我印象中，过年前那几天，DeepSeek 就开始火起来了，当时我在家带娃，没有什么时间上网，但也能感受到 DeepSeek 的渗透。后来我查了一下，DeepSeek 是 1 月 11 号正式发布的，不到半个月的时间就家喻户晓，成为一款 “国民 AI 模型”。经过这段时间的使用和学习 DeepSeek，接下来谈谈我的一些看法。\n模型本身的强大 普通人对大模型的认知，感觉短短两三年时间就从 “超出认知范围” 变成 “全民 AI” 的状态了。简单回顾一下，2022年底 ChatGPT 横空出世，紧接着 Gemini、Claude、Midjourney、Stable Defussion 等国外大模型也持续发力，国内有豆包、kimi等，还有很多很多我没有听说过的。不管是国外还是国内的大模型，这些模型都没有像 DeepSeek 渗透得这么彻底，我还记得 ChatGPT 刚发布没多久，我也是激情澎湃，还特意花 20 美元开了一个月会员，后来热情慢慢退去，再到后来使用频率越来越少了。这几天深入了解 DeepSeek 后，我认为它的强大之处在于：\n高性价比，用更少的算力，达到其他模型的效果，因此 DeepSeek 的定价会更低： DeepSeek-V3 多项评测成绩超越了 Qwen2.5-72B 和 Llama-3.1-405B 等其他开源模型，并在性能上和世界顶尖的闭源模型 GPT-4o 以及 Claude-3.5-Sonnet 不分伯仲。\n随着性能更强、速度更快的 DeepSeek-V3 更新上线，我们的模型 API 服务定价也将调整为每百万输入 tokens 0.5 元（缓存命中）/ 2 元（缓存未命中），每百万输出 tokens 8 元，以期能够持续地为大家提供更好的模型服务。\n上面是 DeepSeek V3 模型的性能测试和价格对比图，总结一下就是：又能打又便宜。\n推理模型+开源 2025 年 1 月 20日，DeepSeek 推出了 DeepSeek-R1 模型，并宣布开源此模型。\n此次我们的开源仓库（包括模型权重）统一采用标准化、宽松的 MIT License，完全开源，不限制商用，无需申请。\nDeepSeek-R1 是一款推理能力强的大模型，性能对齐 OpenAI-o1 正式版。\nDeepSeek-R1 在后训练阶段大规模使用了强化学习技术，在仅有极少标注数据的情况下，极大提升了模型推理能力。在数学、代码、自然语言推理等任务上，性能比肩 OpenAI o1 正式版。\n从使用体验上，个人感觉 DeepSeek-R1 模型让我们从 prompt 工程师转变成甲方，以往的大模型需要各种引导，努力钻研提示词才能得到满意的答案，而 DeepSeek-R1 模型只需要向它清晰地描述需求即可，这样使得它更普惠，普通老百姓也能低门槛使用。\n由于 DeepSeek-R1 模型的开源，各家 AI 公司纷纷独立部署，推出 “满血版” DeepSeek-R1 模型，分担了官方的压力，同时也提高了 DeepSeek 的知名度。\n接地气，时势造英雄 说来惭愧，我第一次听说 DeepSeek 是我的妻子告诉我的，过年前的某一天，我的妻子突然很兴奋的跟我说它用 DeepSeek 解决了某某问题，感觉很强大，极力推荐我去试试。随后我就立即去下载 App，这款国产大模型使用上完全没有什么门槛，而且一推出就是完全免费使用。第一次使用给我眼前一亮的是，这个模型把思考过程都展示出来了，而且回答的内容很接地气。不久后，微信、小红书、B 站、抖音等等各种平台都出现了 DeepSeek 相关的内容，这些内容如洪水猛兽般冲进了我们的视野。\n一款大模型的流行，往往会滋生各种 “搞钱” 教程，各大平台开始出现铺天盖地的利用 DeepSeek 搞钱的教程，结合这几年市场日渐下滑的大形势，很多失业的人仿佛抓到了救命稻草，开始各种学习和推广 DeepSeek，我们抛开 “搞钱” 这个话题，至少学习和使用 DeepSeek 能缓解一部分人群的焦虑，这也给 DeepSeek 添了一把火。\n各种推手 除了各家 AI 公司独立部署的推波助澜以及各种 “搞钱” 教程的推动外，学术研究者们也出了一份力，最具代表性的就是有清华大学背书的一系列研究资料：\n如图是我下载的学习资料，网上还有相对应的各种视频资源，甚至还有人通过卖这些资料来赚钱。这些 “权威” 的学习资料，激起了人们深入探索 DeepSeek 的热情。\n对 DeepSeek 应该抱有怎样的心态 相比其他大模型，我从来没有像 DeepSeek 这样去深入了解和学习过。通过这段时间的使用和学习，我对 DeepSeek 的心情从一开始的兴奋到慢慢平静下来，我觉得不管是大模型的发展，还是个人的成长，都是一条有起伏的曲线，最难能可贵的是不忘初心，持之以恒。\n学习使用，但要避免过度依赖 在 ”全民 DeepSeek“ 的情况下，使用和学习 DeepSeek 肯定是很有必要的，但是不能把它当成无所不能的利器，更不能把它当做逆天改命的神器。不管从事什么工作，过度神话过度依赖某个工具都不是很明智的做法。DeepSeek 只是众多 AI 工具中的一款，未来肯定还会有各种强大的 AI 工具诞生，工具的本质是辅助，使用工具的人才是创世者。\n提升内核，不忘初心 AI 对各行各业的确产生了很大的冲击，“AI 取代员工” 也不完全是危言耸听，但永远不要忘了人类才是创世主， 世间万物每时每刻都在变化，不管外界如何变化，我们的 “魂” 不能变。每个人有每个人的 “魂”，干一行有一行的使命，车夫的使命是把乘客安全送达目的地，即使现在有了飞机、汽车、高铁，相关工作人员的使命也没有变过。与其担心被 AI 干掉，不如想想如何让 AI 帮助自己更好地完成使命，让 AI 帮助自己提升认知，磨练技能。AI 不是敌人，懒惰和安于现状才是。\n参考资料 清华大学公布的相关研究资料 https://api-docs.deepseek.com/zh-cn/news/news1226 https://api-docs.deepseek.com/zh-cn/news/news250120 ","date":"2025-02-23T11:13:00+08:00","permalink":"https://www.blackmatch.cn/p/%E5%85%B3%E4%BA%8E-deepseek-%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/","title":"关于 DeepSeek 的一些思考"},{"content":"前言 Node.js 的模块化是基于 CommonJS 规范实现的，我们通常会使用 module.exports 来导出一个模块，用 require 来引入一个模块。其实在 Node.js 中，一个文件就是一个模块，更多时候我们使用 require 来引入一些 NPM 包。例如：\n1 2 3 const _ = require(\u0026#39;lodash\u0026#39;) // codes 但是有时候我们也需要引入一些文件，最常见的文件就是 .json 文件，例如：\n1 2 3 const package = require(\u0026#39;./package.json\u0026#39;) // codes 用这种方式引入单个文件在大多数情况下是可行的，但是如果引入的文件是一个可能会在程序启动后发生变化的文件就会有问题了。\nrequire 的缓存机制 当程序启动后， Node.js 会在当前进程缓存所有用 require 引入过的内容，并保存在全局对象 require.cache 中。所以，如果使用 require 引入一个动态文件，在程序运行过程中就无法获取最新的文件内容了。\n我们来看一个测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const fs = require(\u0026#39;fs\u0026#39;) const test = () =\u0026gt; { // 首次引用 config.json const config1 = require(\u0026#39;./config.json\u0026#39;) console.log(\u0026#39;first require config\u0026#39;) console.log(config1) console.log(\u0026#39;require cache:\u0026#39;) console.log(require.cache) // 修改 config.json const str = fs.readFileSync(\u0026#39;./config.json\u0026#39;, \u0026#39;utf8\u0026#39;) const obj = JSON.parse(str) obj.age = 20 fs.writeFileSync(\u0026#39;config.json\u0026#39;, JSON.stringify(obj, null, 2)) // 第二次引用 config.json const config2 = require(\u0026#39;./config.json\u0026#39;) console.log(\u0026#39;second require config\u0026#39;) console.log(config2) } test() config.json 文件中的内容为：\n1 2 3 4 { \u0026#34;name\u0026#34;: \u0026#34;blackmatch\u0026#34;, \u0026#34;age\u0026#34;: 18 } 输出结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 first require config { name: \u0026#39;blackmatch\u0026#39;, age: 18 } require cache: { \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/test.js\u0026#39;: Module { id: \u0026#39;.\u0026#39;, exports: {}, parent: null, filename: \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/test.js\u0026#39;, loaded: false, children: [ [Object] ], paths: [ \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/blackmatch/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/node_modules\u0026#39;, \u0026#39;/Users/node_modules\u0026#39;, \u0026#39;/node_modules\u0026#39; ] }, \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/config.json\u0026#39;: Module { id: \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/config.json\u0026#39;, exports: { name: \u0026#39;blackmatch\u0026#39;, age: 18 }, parent: Module { id: \u0026#39;.\u0026#39;, exports: {}, parent: null, filename: \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/test.js\u0026#39;, loaded: false, children: [Object], paths: [Object] }, filename: \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/config.json\u0026#39;, loaded: true, children: [], paths: [ \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/require-cache-demo/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/blackmatch/demos/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/blackmatch/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/Desktop/node_modules\u0026#39;, \u0026#39;/Users/blackmatch/node_modules\u0026#39;, \u0026#39;/Users/node_modules\u0026#39;, \u0026#39;/node_modules\u0026#39; ] } } second require config { name: \u0026#39;blackmatch\u0026#39;, age: 18 } 从输出结果可以看出：\nconfig.json 文件被缓存到了 require.cache 全局对象中了。 在 config.json 文件被修改后，第二次使用 require 引入文件无法获取该文件最新的内容。 这就是 require 的缓存机制。\n使用读取文件的方式引入动态文件 同样基于上面的例子，我们使用读取文件的方式引入 config.json，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 const fs = require(\u0026#39;fs\u0026#39;) const test2 = () =\u0026gt; { // 首次引用 config.json const str1 = fs.readFileSync(\u0026#39;./config.json\u0026#39;, \u0026#39;utf8\u0026#39;) const config1 = JSON.parse(str1) console.log(\u0026#39;first require config\u0026#39;) console.log(config1) // 修改 config.json const str = fs.readFileSync(\u0026#39;./config.json\u0026#39;, \u0026#39;utf8\u0026#39;) const obj = JSON.parse(str) obj.age = 20 fs.writeFileSync(\u0026#39;config.json\u0026#39;, JSON.stringify(obj, null, 2)) // 第二次引用 config.json const str2 = fs.readFileSync(\u0026#39;./config.json\u0026#39;, \u0026#39;utf8\u0026#39;) const config2 = JSON.parse(str2) console.log(\u0026#39;second require config\u0026#39;) console.log(config2) } test2() 输出结果：\n1 2 3 4 first require config { name: \u0026#39;blackmatch\u0026#39;, age: 18 } second require config { name: \u0026#39;blackmatch\u0026#39;, age: 20 } 这次就能获取 config.json 最新的内容了。\n总结 引入一个动态文件的场景比较少，但养成使用读取文件的方式来引入单个文件是个好习惯。 由于 require 的缓存机制，在需要热更新的场景可能需要另辟蹊径，必要时重启进程。 ","date":"2019-06-11T22:04:28+08:00","permalink":"https://www.blackmatch.cn/p/%E8%AF%B7%E4%B8%8D%E8%A6%81%E4%BD%BF%E7%94%A8-require-%E5%BC%95%E5%85%A5%E5%8D%95%E4%B8%AA%E6%96%87%E4%BB%B6/","title":" 请不要使用 require 引入单个文件 "},{"content":"前言 Node.js 使用的是 Google 的 V8 作为 JavaScript 脚本引擎，由于 V8 引擎的限制， Node.js 中只能使用部分内存：64位操作系统下约为1.4G，32位操作系统下约为0.7G。对于浏览器来说，这样的限制影响不大，但是对于服务端程序来说有时候可能就不能满足需求了。虽然大内存的使用场景也相对较少，但还是会存在一些默认情况下非代码因素造成的内存溢出问题。\n通用的解决方案 在网上搜到的大部分答案都是使用 --max_old_space_size 这个 flag 来扩展 Node.js 可使用的最大的老生代的内存。比如：\n1 node --max_old_space_size=2048 xxx.js 单位是 MB，比如上述例子是将最大可用内存扩展至 2G。\n如果你使用的是 Node8.x 及以上的版本，还可以通过 NODE_OPTIONS 这个系统环境变量来配置，例如：\n1 export NODE_OPTIONS=--max_old_space_size=2048 必须要确认该系统环境变量已经生效了，可以用 echo $NODE_OPTIONS 来校验。\n此外，使用Buffer不受 V8内存限制。\n验证扩展内存是否生效 通过 v8 模块的 getHeapStatistics() 方法即可。\n1 2 3 const v8 = require(\u0026#39;v8\u0026#39;) console.log(v8.getHeapStatistics()) 执行 node --max_old_space_size=2048 test.js 结果如下：\n1 2 3 4 5 6 7 8 9 10 11 { total_heap_size: 5009408, total_heap_size_executable: 1048576, total_physical_size: 3449648, total_available_size: 2194521344, used_heap_size: 2411032, heap_size_limit: 2197815296, malloced_memory: 8192, peak_malloced_memory: 410288, does_zap_garbage: 0 } 其中的 heap_size_limit 就是老生代可以使用的最大内存，单位是 Byte。\n注意： Node8.x 以下版本使用这个方法有 bug：当设置的内存超过 3.7G 时，打印出来的结果是有问题的。\n一些特殊情况 在实际的项目中，一般不会很简单的执行 node xxx.js 来启动项目，有些可能通过 pm2 等工具来部署，有些可能通过自己写的 npm 命令行工具来启动，而恰好这个时候使用的 Node.js 版本是低于 8.x 版本，这种情况下就只剩下添加 --max_old_space_size 这个 flag 来扩展内存了，但是怎么加还是有讲究的，比如一个 npm 命令行模块的入口文件的第一行通常是：\n1 #!/usr/bin/env node 这样子的，所以我们很容易就想到：\n1 #!/usr/bin/env node --max_old_space_size=2048 通过这样的方式来扩展内存，但事实上这种方式是有很大的局限性，甚至还有可能导致程序卡死。比如如果源代码这样写：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env node --max_old_space_size=2048 const program = require(\u0026#39;commander\u0026#39;) program .version(\u0026#39;0.1.0\u0026#39;) .command(\u0026#39;start\u0026#39;) .action(function () { // start process... }) program.parse(process.argv) ... 假设我们这个包名叫 blog，在服务器上全局安装这个包后执行 blog start 命令就能将程序启动起来。\n但是，事与愿违，其实在服务器上执行 blog start 的时候，真正执行的是 blog --max_old_space_size=2048，start 这个参数无法传递下去，因为已经被 --max_old_space_size=2048 这个 flag 占用了。这是 #! 写法的一个特性。\n使用 hack 技巧解决特殊情况 针对上述的特殊情况，曾经有人向 Node.js 提过一个 PR，但是后来发现，不需要 Node 做任何修改，使用 shell 的 hack 技巧就能解决，具体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/sh \u0026#34;:\u0026#34; //# comment; exec /usr/bin/env node --max_old_space_size=2048 \u0026#34;$0\u0026#34; \u0026#34;$@\u0026#34; const program = require(\u0026#39;commander\u0026#39;) program .version(\u0026#39;0.1.0\u0026#39;) .command(\u0026#39;start\u0026#39;) .action(function () { // start process... }) program.parse(process.argv) ... 解释一下：\n第一行是指定使用 shshell。 第二行其实分为两个部分：\u0026quot;:\u0026quot; //# comment; 和 exec /usr/bin/env node --max_old_space_size=2048 \u0026quot;$0\u0026quot; \u0026quot;$@\u0026quot;。前半部分是 Bourne Shell 内置支持的写法，只需要了解 # 和 ; 之间是可以写注释的就行了；后半部分是使用 Node 执行相关的命令，$0 传递一个参数 0 ，$@ 传递其他额外的参数。 这样，就能正常传递参数了。\n参考资料 http://sambal.org/2014/02/passing-options-node-shebang-line/\n","date":"2019-06-10T22:02:38+08:00","permalink":"https://www.blackmatch.cn/p/node.js-%E4%B8%AD%E6%89%A9%E5%B1%95%E5%86%85%E5%AD%98%E9%82%A3%E4%BA%9B%E4%BA%8B/","title":"Node.js 中扩展内存那些事 "},{"content":"前言 MySQL 中的大小写敏感问题，可以从服务器 (Server)、数据库 (Database)、表 (Table)、字段 (Column) 这 4 个级别来配置， MySQL 的很多配置、操作等都是基于这 4 个级别的。这4个级别的优先级为：字段\u0026gt;表\u0026gt;数据库\u0026gt;服务器。而我们最关注的大小写敏感问题通常是对于数据而言的，举个简单的例子：字符串 book 和字符串 Book 存入到数据库后，我们通过 SQL 语句查询的时候，如果这两个字符串是相等的，那么说明 MySQL 比较这两个字符串的时候采用了 大小写不敏感 的方式，反之，则是使用了 大小写敏感 的方式。在讨论 MySQL 大小写敏感问题之前，我们需要先了解 MySQL 的两个概念：字符集 (CHARACTER SET) 和校对规则 (Collation)。\n字符集 (CHARACTER SET) 和校对规则 (Collation) 字符集的通常解释是：符号和编码的集合。举个例子：假设我们有 4 个字母： A,B,a,b。我们给每个字母编个号码： A=0,B=1,a=3,b=4 。我们就可以说字母 A 是符号，编号 0 是它的编码。所有的字母和它们的编码组合起来就是我们通常说的字符集。如果我们想要比较两个字符串 A 和 B 的值的大小，那么我们很容就想到他们的编码， A 的编码是 0 ， B 的编码是 1 ，因为 0小于1，所以 字符串 A 小于字符串 B。我们做的这个比较就是给这个 字符集(CHARACTER SET) 应用了一套 校对规则(Collation)。\nMySQL 拥有一套完整的字符集和校对规则。每一种字符集至少包含一种校对规则，且每种字符集都有默认的校对规则。\n可以使用 SHOW CHARACTER SET 语句查看 MySQL 支持的字符集列表： 可以使用 SHOW COLLATION 语句查看 MySQL 支持的校对规则： 大小写敏感问题 接着上面的例子，如果我们想要大写字母 A 和小写字母 a 是相等的呢？那么我们需要做的就是将大写字母 A 和小写字母 a 的编码设置为一致的，然后比较它们的编码即可。这就是我们通常说的 大小写不敏感校对规则。反之，如果将大写字母 A 和小写字母 a 视为不相等的，就是我们通常说的 大小写敏感校对规则。\n在不指定字符集和校对规则的情况下， MySQL 会使用默认的字符集 (utf8) 和校对规则 (utf8_general_ci)。校对规则的名称遵循规则：以其相关的字符集开头，后加上一个或者多个后缀用于区分不同的校对规则。相关后缀说明：\n后缀 含义 _ai 口音不敏感 (Accent insensitive) _as 口音敏感 (Accent sensitive) _ci 大小写不敏感 (Case insensitive) _cs 大小写敏感 (case-sensitive) _ks 假名敏感 (Kana sensitive) _bin 二进制 (Binary，大小写敏感 ) 由此可以知道：默认情况下，MySQL 所使用的校对规则是大小写不敏感的。\n设置大小写敏感 在日常的开发场景中，可能很少遇到需要设置大小写敏感的场景，而我恰巧遇到了。下面举例说明：\n先创建一张用户表： 1 2 3 4 5 CREATE TABLE `users` ( `username` varchar(32) NOT NULL, `nickname` varchar(32) DEFAULT NULL, PRIMARY KEY (`username`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 这里我们先使用默认的字符集和校对规则。\n插入两条数据： 1 2 INSERT INTO `cs_test`.`users` (`username`, `nickname`) VALUES (\u0026#39;user1\u0026#39;, \u0026#39;black\u0026#39;); INSERT INTO `cs_test`.`users` (`username`, `nickname`) VALUES (\u0026#39;user2\u0026#39;, \u0026#39;match\u0026#39;); 查询数据： 1 2 SELECT * FROM users WHERE username=\u0026#39;user1\u0026#39;; SELECT * FROM users WHERE username=\u0026#39;usER1\u0026#39;; 这两条 SQL 语句的查询结果是一致的：\n现在我需要向users表中插入一条username为useR1的另外一个用户，发现报错了： 1 INSERT INTO `cs_test`.`users` (`username`, `nickname`) VALUES (\u0026#39;useR1\u0026#39;, \u0026#39;whatever\u0026#39;); 理由很简单：因为 MySQL 此时使用的是大小写不敏感的校对规则，所以 user1 和 useR1 是相等的，而 username 是 users 表的 主键，所以在插入数据时会报主键冲突的错误。\n设置users表的username字段使用大小写敏感 的校对规则： 1 2 ALTER TABLE `cs_test`.`users` CHANGE COLUMN `username` `username` VARCHAR(32) CHARACTER SET \u0026#39;utf8\u0026#39; COLLATE \u0026#39;utf8_bin\u0026#39; NOT NULL ; 然后再次尝试插入数据，这次插入成功了。此时 users 表中的数据如下：\n再次查询用户表： 发现这次 users 表的 username 字段已经彻底变成了 大小写敏感 了。\n总结 本文讨论的是 MySQL 数据校对规则的大小写敏感问题，而不是讨论表名的大小写敏感问题（网上很多文章都是讨论表名的大小写敏感问题）。 对 MySQL 的字符集和校对规则有一定的了解后有助于理解大小写敏感问题。 MySQL 的 utf8 字符集并没有 utf8_general_cs 校对规则，网上有些文章存在误导。 MySQL 的 utf8_bin 校对规则是大小写敏感的。 MySQL 的 4 个级别优先级为：字段 (Column)\u0026gt; 表 (Table)\u0026gt; 数据库 (Database)\u0026gt; 服务器 (Server)，进行相关配置时尤其要注意。 最好不要对主键等其他含有索引的字段设置大小写敏感，容易导致索引失效，从业务角度考虑也不太合理。 ","date":"2019-04-23T22:00:11+08:00","permalink":"https://www.blackmatch.cn/p/%E6%B5%85%E8%B0%88-mysql-%E4%B8%AD%E7%9A%84%E5%A4%A7%E5%B0%8F%E5%86%99%E6%95%8F%E6%84%9F%E9%97%AE%E9%A2%98/","title":" 浅谈 MySQL 中的大小写敏感问题 "},{"content":"前言 Linux 有很多个发行版本，不同的发行版本有不同的包管理工具。为了安装指定的 Node.js 版本，有时候需要花一些精力找攻略或者安装额外的包管理工具等，有些包管理工具并没有最新的 Node.js 版本。所以，如果是 Linux 系统，索性直接使用编译好的二进制文件进行安装是最省心省力的。\n安装 下载指定版本的二进制文件 在 Node.js 官方的发布网站 https://nodejs.org/dist/ 下载合适的二进制包，比如我要安装 v11.14.0 版本，我需要下载二进制包为 node-v11.14.0-linux-x64.tar.gz。\n解压文件 1 tar -xvf node-v11.14.0-linux-x64.tar.gz 拷贝文件到指定目录 1 sudo cp -r node-v11.14.0-linux-x64/* /usr/local/ 测试是否安装成功 1 2 3 4 5 6 root@blackmatch:~# node -v v11.14.0 root@blackmatch:~# npm -v 6.7.0 root@blackmatch:~# npx -v 6.7.0 总结 二进制包一定要下载合适的（比如 x64 、 x86 ）等。 安装完成后如果相关命令不生效，请重新打开一个终端即可生效。 ","date":"2019-04-16T21:57:25+08:00","permalink":"https://www.blackmatch.cn/p/%E5%9C%A8-linux-%E7%B3%BB%E7%BB%9F%E4%B8%8A%E9%80%9A%E8%BF%87%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E5%AE%89%E8%A3%85-node.js/","title":" 在 Linux 系统上通过二进制包安装 Node.js"},{"content":"需求 ![requirement]({{ site.url }}/images/oracle-blob-buffer/requirement.jpg)\n源库中的数据是以 BLOB 的形式存储的，且数据中含有中文， MySQL 数据库的字符集为 utf8，最终想要的效果就是在浏览器中以文本的形式展示源库中的数据。为了实现这一需求，尝试了 2 种方案：\n从 Oracle 层面解决，通过视图将相关字段转换成 VARCHAR2 类型后在返回，这样从 Oracle 中查询数据的时候，直接拿到的就是字符串类型的数据。这样做的弊端是： Oracle 数据库 VARCHAR2 类型最大只能支持 4kb，如果超过了这个大小就会出错。 从 Oracle 取到数据后，使用 Node.js 转换成字符串后再存入到 MySQL 数据库中。 我使用了第 2 种解决方案，但是过程并不是很顺利。\n遇到的问题 从 Oracle 数据库中取到的数据，在 Node.js 中是 Buffer 对象，要将 Buffer 对象转换成字符串对 Node.js 来说实在是太常规了，直接 buffer.toString 就完事了，可事实并非如此，得到的字符串都是乱码。一般遇到这个问题，大家的第一反应肯定是编码问题，我也是这么想的，考虑到数据中有中文，而 Node.js 原生并没有支持中文的相关编码，默认是 utf8，已经尝试过了。所以就引入了 iconv-lite 这个模块，用来对 Buffer 对象进行解码，但是 Oracle 中使用的字符集是 SIMPLIFIED CHINESE_CHINA.ZHS32GB18030，所以我想当然的就使用 GB18030 编码来解码，代码示例：\n1 2 3 4 const iconv = require(\u0026#39;iconv-lite\u0026#39;); // Convert from an encoded buffer to js string. const str = iconv.decode(buffer, \u0026#39;gb18030\u0026#39;); 结果得到的字符串还是乱码，然后我又把 iconv-lite 支持的所有中文编码又试了一遍，得到的字符串全都是乱码。\n解决 经过一番 Google 和尝试后仍然没有解决，然后就在上述提到的两种方案之间来回折腾。后来在朋友的引导下，得到了一个思路：先探测 Buffer 对象的编码，得到确定的编码后，再进行解码。于是乎就找到了这个模块：detect-character-encoding。这个模块主要是用来探测字符编码的，使用方法也很简单，示例代码：\n1 2 3 4 5 6 7 8 9 10 11 const fs = require(\u0026#39;fs\u0026#39;); const detectCharacterEncoding = require(\u0026#39;detect-character-encoding\u0026#39;); const fileBuffer = fs.readFileSync(\u0026#39;file.txt\u0026#39;); const charsetMatch = detectCharacterEncoding(fileBuffer); console.log(charsetMatch); // { // encoding: \u0026#39;UTF-8\u0026#39;, // confidence: 60 // } 于是乎就用这个模块对上述提到的 Buffer 对象进行探测，得到的编码竟然是 UTF-16LE，然后使用这个编码进行解码，果然得到了正确的字符串。问题到此彻底解决了。\n注意事项 探测编码时请多用一些数据样例来探测，最后使用可信度最高的编码。 千万不要动态探测编码，然后动态解码，因为这个模块的探测结果是随着数据的变化而变化的。 使用 iconv-lite 模块解码时，如果编码名称中有字母，请一律使用小写字母。 一定要确保从 Oracle 取到的数据在 Node.js 环境中为 Buffer 对象。 其他说明 连接 Oracle 使用的模块是 oracledb 连接 MySQL 使用的模块是 knex 总结 这次遇到的问题，其实解决方案是比较清晰的，但是在对 Buffer 进行解码遇到问题后没有冷静下来分析，在 2 个解决方案之间来回折腾浪费了很多时间；当已经很明确问题出现在哪个环节时，应该借助相关工具进一步确认问题的根源所在，比如：这次在解码环节出现了问题，而问题的根源也比较清晰，就是解码时使用的编码不对，所以就应该先明确 Buffer 对象所使用的编码，然后再用正确的编码进行解码即可。\n","date":"2019-03-12T21:54:03+08:00","permalink":"https://www.blackmatch.cn/p/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BB%8E-oracle-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%96-blob-%E6%95%B0%E6%8D%AE%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","title":" 记一次从 Oracle 数据库取 BLOB 数据遇到的坑 "},{"content":"转眼间， 2018 年马上就要过去，以前从来没有总结自己过去一年的习惯，那就从今年开始吧。我对自己 2018 年的总体表现是：不太满意。在技术上虽然有了比较明确的方向和目标，但是大部分都没能达到理想的结果。原因就只有一个字：懒！但不管怎样，人总是会随着时间的推移不断成长的，所以经常对自己的过去进行反思和总结也是有必要的。只有更加认清自己，才能走得更快走得更远。所以我主要想好好总结一下自己这一年的成长和不足。主要分为两方面：技术和其他软技能。\n技术成长 我对自己 2018 年的期望是：在 Node.js、后端、架构能力上有较大的提升。首先聊聊 Node.js，毕竟这是我吃饭的技术。感觉自己今年在 Node.js 上没有多大的进步，除了工作中使用到的技术比较熟悉外，其他方面没有深入去学习，工作中使用的技术也比较有局限性。对 Node.js 底层的东西没有入去学习，比如 事件循环、Emitter、Stream、V8 等，虽然在工作中能熟练运用，但对原理却说不出个所以然。其次是后端能力，这一年 主要提高的是数据库方面的能力，对 SQL 性能优化、 MySQL、 DB2 有了更深入的理解，在 SQL 性能调优方面积累了一些经验，但是还有很多需要学习，当前对我来说最重要的就是服务器性能监控和调优以及前后端交互优化方面的技术。最后是架构能力，算是小有收获吧。至少对一些架构概念有了初步的了解，比如 高可用、高性能、双机架构、架构相关方法论 等。还是罗列一下我在 2018 年在工作之余都做了什么吧。代码相关的大部分都放在 GitHub 了，这一年主要写了这些：\n![github-contribute-2018]({{ site.url }}/images/goodbye-2018/github-contribute-2018.jpg)\n用 Node.js 写了一个少儿不宜的爬虫 :joy: ，项目地址：blackmatch/pornhub-downloader，这个竟然还是 star 最多的。。。真是无心插柳柳成荫，该项目更新到第二个版本已经停止维护的，毕竟我的初心是练习 Node.js 相关技术，确实也是这个项目学到的东西最多，比如 HTTP 代理、 Range 请求等。 写了一个生成我国最新省市县行政区域的工具，项目地址：blackmatch/mockz，踩了一些高并发请求的坑。 翻译了两篇Node.js 最佳实践 的文章，项目地址：i0natan/nodebestpractices，这是一个很好的项目，里面有很多 Node.js 最佳实践的文章。 其他还有一些杂七杂八不值一提的东西，比如使用 docker 搭建 gitlab、 gogs 等教程。 当然，这些都是一些很微不足道的东西，但是总比什么都不做好。至少慢慢养成了动手实践，坚持写作的习惯。虽然写的东西都是很轻量的，但是其中也会遇到一些棘手的问题，如果能寻根问题，也能学到很多。其实我写的这些项目中还有一些没有解决、没有弄懂的问题，所以，把每一件小事做到极致也能有所收获。\n其他软技能 程序员除了在技术上要有一技之长，软实力也不可小觑。比如：沟通能力、协调能力、学习方法、眼界等。在这一年，自己的学习方法和看问题的深度有了一定的提高。学习方法都是因人而异的，有人喜欢直接刚源码，有人喜欢看视频，有人喜欢看书看文档看别人的代码等，找到适合自己的方法就是最好的方法，最关键的一点是：要行动起来。我比较喜欢的学习方法是：先过一遍官方文档，然后看看视频，然后做一些小的功能或者项目，逐渐深入。程序员在项目中遇到问题时，思考的方向容易受到束缚，很容易就往技术方向去思考，比如针对新需求或者需求变更时，往往会先从技术实现上先想一遍，然后才会关注需求的合理性和相关细节，这样会一定程度上带入自己的主观意识，甚至和产品干起来。所以，思考问题时，要学会多换位思考，不要被技术束缚了，不要忘了：技术是用来满足需求的。\n展望未来 程序员几乎无一例外的都需要面临一个问题：技术的日新月异。唯一的解决方法就是：不断学习，不断成长。在即将到来的 2019 年，我不会给自己立什么 flag，只希望自己 不要太懒，朝着目标和方向默默努力就行了，行动才是成败的关键。共勉。\n","date":"2018-12-30T21:51:55+08:00","permalink":"https://www.blackmatch.cn/p/%E5%86%8D%E8%A7%81-2018/","title":" 再见 2018"},{"content":"前言 momentjs 是一个很流行的处理日期、时间的库，这个库提供了很多友好的方法，方便开发者对日期时间进行比较、格式化等操作。然而，由于这个库是基于面向对象的思维逻辑设计的，每一个 moment 对象会自带很多属性和方法，在进行相关的操作时会进行很多校验、转换等操作，所以在对性能比较敏感的程序中可能会存在一定的问题，特别是在循环语句中，性能问题可能更加突出。\n我在实际的项目中就遇到过在循环语句中使用 momentjs 导致性能问题的情景：从一个数组中取出符合条件的元素，条件之一是需要对元素的某个日期时间属性作判断，使用到了 momentjs 的 .isAfter 方法，数组中有 4 百多个元素，每次遍历一个元素需要 80ms，所以完成整个遍历需要 30 多秒，虽然使用场景不是提供 API 接口，但这也是很难接受的。后来换成了 JavaScript 原生的方法，每遍历一个元素只需要不到 2ms，遍历所有的元素不到一秒就完成了，性能瞬间就提升了。说明：我仅仅是修改了日期时间比较的代码，其他没有做任何优化。\n简单测试 我们可以简单的测试一下 momentjs 和原生的 JavaScript 方法之间的性能差异，测试代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 const debug = require(\u0026#39;debug\u0026#39;)(\u0026#39;moment-demo\u0026#39;); const moment = require(\u0026#39;moment\u0026#39;); const items = []; for (let i = 0; i \u0026lt; 500; i += 1) { const ts = new Date().getTime(); const n1 = Math.floor(Math.random() * 1000); const n2 = Math.floor(Math.random() * 1000); items.push({ n1, n2, time1: new Date(ts - n1 * 24 * 60 * 60 * 60), time2: new Date(ts - n2 * 24 * 60 * 60 * 60) }); } debug(`items len: ${items.length}`); const arr1 = []; for (let i = 0, iLen = items.length; i \u0026lt; iLen; i += 1) { const item = items[i]; if (moment(item.time2).isAfter(item.time1)) { arr1.push(item); } } debug(`arr1 len: ${arr1.length}`); const arr2 = []; for (let i = 0, iLen = items.length; i \u0026lt; iLen; i += 1) { const item = items[i]; const t1 = new Date(item.time1).getTime(); const t2 = new Date(item.time2).getTime(); if (t2 \u0026gt; t1) { arr2.push(item); } } debug(`arr2 len: ${arr2.length}`); 测试结果如下：\n![debug]({{ site.url }}/images/you-dont-need-momentjs/debug.jpg)\n从结果可以看出，momentjs 在性能上确实逊色于原生的 JavaScript 方法。\n相关库的比较 其实， github 上已经有一个项目专门分析 momentjs 与其他日期时间处理库的各项对比，项目地址在 这里 。一个简要对比如图：\n![vs]({{ site.url }}/images/you-dont-need-momentjs/vs.jpg)\n我的建议 如果你只是用 momentjs 中的两三个方法，则完全可以考虑使用其他更加轻量、性能更加好的库，或者使用 JavaScript 原生的方法，必要时封装一下即可。如果程序对性能有较高的要求，最好不要使用 momentjs。\n","date":"2018-12-17T21:49:43+08:00","permalink":"https://www.blackmatch.cn/p/%E6%88%96%E8%AE%B8%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%B8%8D%E9%9C%80%E8%A6%81-moment.js/","title":" 或许你真的不需要 moment.js"},{"content":"前言 我们可以使用 Node.js 的 http 模块进行网络请求，比如使用 http.get 方法进行 GET 请求。当时在高并发请求的情况下，很容易出现如下的错误：\n1 2 Error: getaddrinfo ENOTFOUND www.baidu.com www.baidu.com:80 at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:57:26) 或者是这样的错误：\n1 2 Error: queryA ETIMEOUT www.baidu.com at QueryReqWrap.onresolve [as oncomplete] (dns.js:197:19) 这两个错误都和 DNS 解析有关。\n一种解决方案 在 Google 搜索一番就会找到如下的解决方案：\n1 2 3 4 5 6 7 8 const opts = { host: \u0026#39;www.baidu.com\u0026#39;, family: 4 }; http.get(opts, (res) =\u0026gt; { // handle the response }) 关键点是在请求的时候添加了 family 这个参数，我们先来看一下这个参数的官方解释：\nfamily IP address family to use when resolving host and hostname. Valid values are 4 or 6. When unspecified, both IP v4 and v6 will be used.\n大概的意思是：这个参数可以指定解析 host 和 hostname 的时候所使用的 IP 地址族。可接受的参数为 4 和 6 。如果不指定，会同时使用 IP-v4 和 IP-v6 。\n所以，这种解决方案就是让 Node 在调用 dns 模块解析域名的时候，指定使用 IP-v4 。这样能够在一定程度上解决问题。为什么说们说是一定程度上呢？因为经过我的验证，当请求并发量继续增大的时，还是会存在问题。\n一些猜想 Node 使用 dns 模块来提供域名解析服务，当我们调用 http 、 net 等相关模块时，也会使用到 dns 模块来进行相关的操作。当然，dns 模块也可以单独使用，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 const dns = require(\u0026#39;dns\u0026#39;); const len = 10000; const run = () =\u0026gt; { for (let i = 0; i \u0026lt; len; i++) { dns.resolve4(\u0026#39;www.baidu.com\u0026#39;, (err, addresses) =\u0026gt; { if (err) { console.log(`i: ${i}`); throw err; process.exit(0); } console.log(addresses[0]); }); } }; run(); 那为什么高并发请求的时候会出现问题呢？我的猜想是：dns 模块对域名解析有一定的性能限制，当并发量达到一定程度时，就会出现超时，从而导致各种问题。那为什么使用 IP-v4 就能得到一定程度的改善呢？我的猜想是：默认情况下，dns 模块使用的是 IP-v4 和 IP-v6 进行域名解析，在切换解析规则时或者使用不同的规则对性能有一定的依赖，当指定使用 IP-v4 的时候，能够使得 dns 模块发挥最佳的性能，从而使问题得到一定的改善。\n一些建议 在一些高并发请求的场景（比如爬虫）下，很有可能会导致 DNS 无法正常解析的问题，比如使用 request 模块进行高并发请求的时候也会出现问题。除了上述提到的解决方案外，还应该合理控制好并发量。此外，也可以尝试使用高性能的 DNS 服务器来提供 DNS 解析的效率。\n参考资料 http://www.ruanyifeng.com/blog/2016/06/dns.html\nhttps://github.com/nodejs/node/issues/1644\n","date":"2018-10-13T21:47:00+08:00","permalink":"https://www.blackmatch.cn/p/%E4%BD%BF%E7%94%A8-node.js-%E8%BF%9B%E8%A1%8C%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AF%B7%E6%B1%82%E6%97%B6%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F-dns-%E8%A7%A3%E6%9E%90%E9%97%AE%E9%A2%98/","title":" 使用 Node.js 进行高并发请求时需要注意 DNS 解析问题 "},{"content":"前言 前段时间，我写了一篇《 使用 Gogs 搭建自己的 Git 服务 》，结果大家都不太看好 Gogs，我在使用 Gogs 几天后，感觉是：搭建简单，对性能要求不高。也正因为简单和消耗资源少， Gogs 慢慢的无法满足日益复杂的 DevOps 需求，比如 CI/CD （ continuous integration and continuous delivery，持续集成和持续交付）、性能问题等。对于 Gogs 和 gitlab 我不作评论，这里有一篇来自 gitlab 官网相当于竞品分析的文章：https://about.gitlab.com/comparison/gogs-vs-gitlab.html。本文主要介绍如何使用 docker-compose 搭建 gitlab 。\n准备工作 关于使用 docker/docker-compose 搭建 gitlab，其实有很多教程了，官方也有相应的介绍。但是，我搜到的很多教程使用的 docker 镜像是来自民间大牛的开源镜像：https://github.com/sameersbn/docker-gitlab，对于有点强迫症的我来说，既然 gitlab 官方提供了镜像，为何不用官方的呢？其次就是， gitlab 官方关于使用 docker-compose 搭建 gitlab 的描述篇幅较少，一些细节问题需要自己实践后才会遇到。下面一步一步介绍我的折腾过程，就当是自己的学习笔记吧。官方的教程在这里：https://docs.gitlab.com/omnibus/docker/#install-gitlab-using-docker-compose。在开始之前，先介绍一下我的装备（穷不是我的错。。。）：\n一台 2012 年买的华硕笔记本，露个脸吧： 这台笔记本是我读大学用的『战机』，被我花巨资改造了一下：内存加到了 8G，换上一个 128G 的固态硬盘。这个笔记本在我读大学的时候已经被折腾过好多次了，自从毕业后就很少用了，所以拿出来在家里当服务器用，装了 win10 、 VirtualBox，然后装了一个 Ubuntu18.04.1 的虚拟机，分了 4G 内存、 40G 硬盘，安装好 docker、 docker-compose。\n一台低配版 Mac mini。 8G 内存、 1T 硬盘，用终端连接 Ubuntu 服务器。题外话：直接在我的 Mac mini 上搭建 gitlab，发现有点带不起来，可能是因为我装了太多软件了。 硬件就这两台电脑，云服务器目前是买不起的，也没有必要。下面开始折腾吧。\n先让 gitlab 跑起来 在 Ubuntu 服务器上拉取 gitlab-ce 官方镜像：\n1 docker pull gitlab/gitlab-ce:11.3.1-ce.0 注意：我使用的 tag 是 11.3.1-ce.0\n新建一个 docker-compose.yml 文件，写入如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 version: \u0026#34;3\u0026#34; services: web: image: \u0026#39;gitlab/gitlab-ce:11.3.1-ce.0\u0026#39; container_name: gitlab environment: GITLAB_OMNIBUS_CONFIG: | external_url \u0026#39;http://git.blackmatch.cn\u0026#39; ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;33:22\u0026#39; volumes: - \u0026#39;./srv/gitlab/config:/etc/gitlab\u0026#39; - \u0026#39;./srv/gitlab/logs:/var/log/gitlab\u0026#39; - \u0026#39;./srv/gitlab/data:/var/opt/gitlab\u0026#39; 保存文件，在该文件目录下执行：\n1 docker-compose up 然后会看到终端上刷刷刷 ~~~ 滚动很多内容，耐心等待一会（我这里大概 3 分钟左右），然后在我的 Mac mini 的浏览器上访问 http://git.blackmatch.cn （我事先已经将域名和 Ubuntu 服务器的 IP 做了 hosts 映射），看到如下界面：\n看到这个界面说明 gitlab 已经成功跑起来了，可以在这个界面设置 root 账号的密码，然后使用 root 账号登录系统：\n登录成功后的界面是这样的：\n我来创建一个项目看看：\n这个新建项目的 HTTP 地址为：http://git.blackmatch.cn/root/test.git，这里顺带提一下，这里的地址中的 git.blackmatch.cn 就是我在 docker-compose.yml 文件中配置的环境变量：\n1 2 3 environment: GITLAB_OMNIBUS_CONFIG: | external_url \u0026#39;http://git.blackmatch.cn\u0026#39; 这里的 external_url 配置的是外部 URL，会影响项目的访问地址，如果不配置，项目的访问地址会是一个随机字符串，在云服务器上搭建时尤其要注意这一点。\n开启邮件服务 我们在使用 github 等类似的平台工具的时候都会用到邮件服务，比如你在 github 上进行注册、密码重置、有人给你的开源项目提 issue 等等的时候，你通常都会收到邮件提醒。 gitlab 肯定也会有这个功能的，下面我们就来开启这个功能，在此之前需要准备一个邮箱账号，这个账号是用来负责发送邮件的，需要开启 smtp 协议支持。使用 CTRL + C 快捷键停止正在运行中的 gitlab 容器，然后修改 docker-compose.yml 为如下内容（这里以 gmail 邮箱为例）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version: \u0026#34;3\u0026#34; services: web: image: \u0026#39;gitlab/gitlab-ce:11.3.1-ce.0\u0026#39; container_name: gitlab environment: GITLAB_OMNIBUS_CONFIG: | external_url \u0026#39;http://git.blackmatch.cn\u0026#39; # email setting gitlab_rails[\u0026#39;smtp_enable\u0026#39;] = true gitlab_rails[\u0026#39;smtp_address\u0026#39;] = \u0026#34;smtp.gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_port\u0026#39;] = 587 gitlab_rails[\u0026#39;smtp_user_name\u0026#39;] = \u0026#34;my.email@gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_password\u0026#39;] = \u0026#34;my-gmail-password\u0026#34; gitlab_rails[\u0026#39;smtp_domain\u0026#39;] = \u0026#34;smtp.gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_authentication\u0026#39;] = \u0026#34;login\u0026#34; gitlab_rails[\u0026#39;smtp_enable_starttls_auto\u0026#39;] = true gitlab_rails[\u0026#39;smtp_tls\u0026#39;] = false gitlab_rails[\u0026#39;smtp_openssl_verify_mode\u0026#39;] = \u0026#39;peer\u0026#39; ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;33:22\u0026#39; volumes: - \u0026#39;./srv/gitlab/config:/etc/gitlab\u0026#39; - \u0026#39;./srv/gitlab/logs:/var/log/gitlab\u0026#39; - \u0026#39;./srv/gitlab/data:/var/opt/gitlab\u0026#39; 将配置中的 my.email@gmail.com 和 my-gmail-password 替换为你自己的邮箱和密码，记得一定要开启邮箱的 smtp 协议服务，否则无法发送邮件。其他邮箱的配置方式大同小异，详细的介绍可以看这里：https://docs.gitlab.com/omnibus/settings/smtp.html。保存配置文件，然后执行 docker-compose up ，然后使用 root 账号登录 gitlab，新建一个账户（注意正确填写新建账户的邮箱地址），新建账户成功后，会给新账户的邮箱发送一封邮件，如下：\n至此，邮件服务已成功启用。\n安装 git-runner 前面我提到， gitlab 支持 CI/CD，如果我们需要对某个项目进行持续集成 / 持续交付，则需要给该项目配置一些任务，当我们每次 push 代码的时候，自动触发这些预先配置好的任务，而这些任务的执行者就是 gitlab-runner。可以简单的概括一下 gitlab 和 gitlab-runner 的关系： gitlab 在监测到有代码提交时，通知 gitlab-runner 去执行一些任务， gitlab-runner 执行完这些任务后将执行的结果反馈给 gitlab。所以，如果要开启 CI/CD，需要先完成两件事： 1. 在服务器上安装好 gitlab-runner； 2. 注册 gitlab-runner，打通 gitlab 和 gitlab-runner 之间的交互。在服务器上安装 gitlab-runner 的步骤如下：\n下载二进制安装包 1 sudo wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 修改权限： 1 sudo chmod +x /usr/local/bin/gitlab-runner 在服务器上新增一个用户（ gitlab-runner），专门用于执行 CI/CD 任务： 1 sudo useradd --comment \u0026#39;GitLab Runner\u0026#39; --create-home gitlab-runner --shell /bin/bash 执行安装： 1 sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner 启动 gitlab-runner： 1 sudo gitlab-runner start 接下来就是注册 gitlab-runner，只有注册之后 gitlab 和 gitlab-runner 之间才能交互。使用 root 账号登录 gitlab 之后，在路由 /admin/runners 下可以看到注册 gitlab-runner 需要用到的 URL 和 token 。如下：\n在服务器上执行：\n1 sudo gitlab-runner register 然后根据提示输入 URL 和 token 等信息就可以了，过程如下：\n好吧。。。报错了，这个可能是我的 URL 是域名，但是服务器没有配置 hosts ，故在服务器上无法通过域名访问 gitlab，修改一下服务器的 /etc/hosts 文件，重新注册一下：\n这回注册成功了，需要注意的是，在注册 gitlab-runner 的时候，需要选择一个或者多个执行器（ executor），不同的执行器有不同的特性和功能，如果你不知道怎么选择，可以先看看这里：https://docs.gitlab.com/runner/executors/README.html#i-am-not-sure。我这里选择了最简单的 shell 执行器，shell 执行器可以让 gitlab-runner 在执行任务的时候直接使用 shell 命令，但是如果用到一些构建工具，必须事先在服务器上安装这些工具，例如：在代码提交后通知 gitlab-runner 执行 npm run build 命令来构建项目，则需要事先在服务器上安装 npm 。 gitlab-runner 注册成功后，刷新一下网页，会看到多了一个 runner：\n这样我们就可以使用这个 runner 为我们执行任务了，这里需要注意的是，新创建的 runner，默认情况下，只对打 tag 的 commit 触发任务，如果想要每一次提交代码都执行，则需要修改 runner 的配置，如下：\n至此， gitlab-runner 就已开启成功了，可以对项目进行 CI/CD 配置了。\n测试 CI/CD 经过前面的部署，目前我搭建的 gitlab 已经可以使用 CI 功能了，这里我简单的测试一下这个功能，前面我们已经使用 root 用户创建了一个 test 项目，将这个项目 clone 到我的 Mac mini 电脑上，然后在项目根目录下分别创建 app.js 和 .gitlab-ci.yml 这两个文件，要启用 gitlab 的 CI 功能，除了需要正配安装并配置好 gitlab-runner 外，还需要在每个项目的根目录下创建一个名为 .gitlab-ci.yml 文件，并在这个文件下配置具体的任务，例如我们可以简单的测试一下 gitlab-runner 是否能正常运行， .gitlab-ci.yml 文件内容如下：\n1 2 3 release: script: - cp ./app.js /home/gitlab-runner/release 这里就设置一个很简单的任务：将项目中的 app.js 文件拷贝到服务器的 /home/gitlab-runner/release 目录下。在我的 Mac mini 上对 test 项目修改的内容提交到服务器，会触发 gitlab-runner 执行任务，结果如下：\n需要注意的是， gitlab-runner 在服务器上执行每个项目预先配置的任务的时候是以 gitlab-runner 用户的身份执行，所以要注意权限的问题，我第一次提交的时候，因为 release 目录是我用服务器的 root 权限创建的，所以 gitlab-runner 用户没有权限将文件复制到该目录下，修改目录所有者后，第二次提交就成功了，如下：\n至此， gitlab-runner 执行 CI 任务就测试成功了。当然这只是一个很简单的测试， gitlab 还有很多关于 CI/CD、 Auto DevOps 等方面的功能特性，对持续集成、持续交付、提升项目质量等方面都有很大的帮助，大家感兴趣可以多去尝试。\n完整的配置 经过上面的配置和调试，目前的 gitlab 已经能满足开发工作流中一些常用的需求，如果没有其他需求，可以直接把该容器投入到生产中，完整的 docker-compose.yml 文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 version: \u0026#34;3\u0026#34; services: web: image: \u0026#39;gitlab/gitlab-ce:11.3.1-ce.0\u0026#39; container_name: gitlab restart: always environment: GITLAB_OMNIBUS_CONFIG: | external_url \u0026#39;http://git.blackmatch.cn\u0026#39; # email setting gitlab_rails[\u0026#39;smtp_enable\u0026#39;] = true gitlab_rails[\u0026#39;smtp_address\u0026#39;] = \u0026#34;smtp.gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_port\u0026#39;] = 587 gitlab_rails[\u0026#39;smtp_user_name\u0026#39;] = \u0026#34;my.email@gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_password\u0026#39;] = \u0026#34;my-gmail-password\u0026#34; gitlab_rails[\u0026#39;smtp_domain\u0026#39;] = \u0026#34;smtp.gmail.com\u0026#34; gitlab_rails[\u0026#39;smtp_authentication\u0026#39;] = \u0026#34;login\u0026#34; gitlab_rails[\u0026#39;smtp_enable_starttls_auto\u0026#39;] = true gitlab_rails[\u0026#39;smtp_tls\u0026#39;] = false gitlab_rails[\u0026#39;smtp_openssl_verify_mode\u0026#39;] = \u0026#39;peer\u0026#39; ports: - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;33:22\u0026#39; volumes: - \u0026#39;./srv/gitlab/config:/etc/gitlab\u0026#39; - \u0026#39;./srv/gitlab/logs:/var/log/gitlab\u0026#39; - \u0026#39;./srv/gitlab/data:/var/opt/gitlab\u0026#39; 保存文件后，使用如下命令启动容器：\n1 docker-compose up -d 至此， gitlab 已成功部署。\n其他需要注意的问题 gitlab 对服务器的性能要求较高，至少要保证有 4G 的内存，否则很容易出现各种奇怪的问题，比如网页 502 等。 gitlab-runner 在执行任务的时候不对你的项目做任何修改，因为其原理是：先把项目所有内容 fetch （默认）或者 clone 到 /home/gitlab-runner 目录下，然后再执行相关的任务操作，不会对你的项目做任何修改。 每次修改完 docker-compose.yml 文件后，重新启动容器可能会花费较多的时间，请耐心等待。 汉化问题，可能较老的 gitlab-ce 版本需要专门下载一个中文包来做汉化，现在只需要在账户登录后，在账户设置中将 Preferred language 设置成 简体中文 即可，保存设置后刷新界面，效果如下： 参考资料 https://docs.gitlab.com/omnibus/docker/#install-gitlab-using-docker-compose\nhttps://docs.gitlab.com/runner/\nhttps://www.jianshu.com/p/2b43151fb92e\nhttps://docs.gitlab.com/omnibus/settings/smtp.html\nhttps://docs.gitlab.com/runner/executors/README.html\n","date":"2018-10-03T21:40:04+08:00","permalink":"https://www.blackmatch.cn/p/%E4%BD%BF%E7%94%A8-docker-compose-%E6%90%AD%E5%BB%BA-gitlab/","title":" 使用 docker-compose 搭建 gitlab"},{"content":"故事背景 我在 vultr 上有一台 VPS，由于服务器在洛杉矶，在国内访问速度比较慢。以前有听说 Google 的 bbr 拥塞算法能够提高 tcp 的传输速度，于是想着把 bbr 打开。我的服务器环境如下：\n操作系统： CentOS7 Linux 内核版本： 3.10.0 SELINUX 配置： disabled docker 版本： 18.06.1-ce 开启 bbr 开启 bbr 需要先把 Linux 内核版本升级到 4.10 以上，所以首先需要升级内核。我是根据这个教程 https://www.vultr.com/docs/how-to-deploy-google-bbr-on-centos-7 一步一步操作的，最后将 Linux 内核版本升级到了 4.18.9 ，最后也成功开启了 bbr，感觉访问速度稍微开了那么一点点。\ndocker 服务无法启动了 开启 bbr 后发现 docker 服务起不来了，尝试过几次删掉重装，结果还是一样，排查得到如下报错：\n1 unable to configure the Docker daemon with file /etc/docker/daemon.json 把这个报错仍 Google，尝试了几个方案都不行。继续折腾，再根据报错信息：\n1 failed to start docker application 继续 Google，在 segmentfault 找到了这个问答：https://segmentfault.com/q/1010000002392472，感觉和我的情况很像啊。暗喜中。。。看到这个问题有好多人回答，我就从上往下一个一个试，其中有个回答是：\n那我就照做 :\n1 2 3 4 5 vim /etc/selinux/config i edit... wq shutdown -r now 一顿操作猛如虎，然后喝口茶等待系统重启。。。等了一两分钟，使用 ssh 连接服务，突然给我来一个惊喜：\n1 permission denied 系统无法登录了 ssh 连不过去了，卧槽这是什么情况。因为我的服务器只有 root 账号，我一直都是用 root 账号登录的，想尝试其他账号也不行了。我意识到肯定是我刚才修改了 SELINUX 配置导致，但是现在登不进去系统想改回来也不行啊。虽然我对 Linux 懂的不多，但是这个时候一定要保持冷静，继续喝一口茶。想到了找学弟帮忙，学弟是做运维的，对 Linux 特别熟，大学时候和他一起培训过几天 Linux。学弟提醒我可以去 VPS 提供商的后台管理界面看看能不能做些什么。于是我登录到 vultr，进入后台管理界面，发现有一个 View Console 操作，如下：\n点击，弹出一个窗口，进入了一个 Linux 终端：\n哈哈！感觉看到了希望，输入账号、密码，用力敲一下回车键，啪！提示：\n1 permission denied 什么鬼？？？结果还是一样啊！还是登不进去啊！冷静冷静。。。继续喝口茶， Google 一番尝试了一些方法，都不行。茶已经喝完啦，脾气也上来了。想着直接把系统重置了吧，反正也没有特别重要的数据。说时迟那时快，当我快要点击重置系统按钮的时候，脑子里突然闪过一个画面，就像动漫主角快要被干掉时突然开挂了一样，这个画面就是：\n这个难道就是传说中的恢复模式？不管了，点一下看看，出现了这个界面：\n卧槽！我感觉还能抢救一下。看个这个，感觉又看到了希望，外挂已上线 ~~ 哈哈哈。说实话，我对这个界面真的不熟，只知道这里有很多个内核，可以选择任何一个系统启动。于是问学弟，结果被学弟鄙视，让我我按照忘记密码的套路去操作，接下来又是一波搜索。经过一番倒腾，终于找到了一个可行的方案，具体操作如下：\n选中一个内核，我这里选择的是我更新后的内核，按下键盘上的 e 键进入编辑模式，然后使用 向下键 一直往后翻，找到 rhgb quiet LANG=en_US.UTF-8 在后面加上 enforcing=0 ，如下：\n然后按下 Ctrl + x 启动系统，正确输入账号、密码，再次用力敲一下回车键，这次终于等进去了！！！总算是抢救回来了，然后里面修改 SELINUX 配置，又是一顿操作猛如虎，重启之后能正常登录了，彻底好了。\ndocker 服务怎么办 虽然已经能够正常登录服务器了，但是 docker 还是老样子起不来。但是，在系统无法登录那一刻，我冥冥之中已经感觉到 docker 的问题应该如何解决了，因为我看到了这个回答：\n![88]({{ site.url }}/images/centos-enable-bbr/88.png)\n我的直觉向来很准的，因为读大学的时候自己经常折腾各种黑科技之类的，这种感觉我很熟悉，哈哈哈。按照这个方法操作，最后 docker 服务真的好了，哈哈哈！\n结束语 本来都已经打算重置系统了，没想到最后 bbr 成开启了， docker 服务也弄好，真是成功逆袭了啊！因为我对 Linux 不是很熟，所以大牛们可能觉得这根本不算是什么事，对我来说那可是相当惊险刺激啊！感谢学弟的耐心指导，我以后一定会努力学习 Linux 的。最后友情提示：\n没事不要升级 Linux 内核！\n没事不要乱改 SELINUX 配置！\n没事不要瞎折腾！\n注意备份重要数据！\n注意备份重要数据！\n注意备份重要数据！\n","date":"2018-09-28T21:37:40+08:00","permalink":"https://www.blackmatch.cn/p/%E5%BC%80%E5%90%AF-bbr-%E5%B7%AE%E7%82%B9%E5%BC%95%E5%8F%91%E4%B8%80%E5%9C%BA%E8%A1%80%E6%A1%88/","title":" 开启 bbr 差点引发一场血案 "},{"content":"什么是 Gogs Gogs 是一款极易搭建的自助 Git 服务。作为程序员，可能对 Github 已经非常熟悉了，我们经常会把自己的代码提交到 Github。非付费用户在 Github 的代码是公开的，任何人都可以看到你提交的代码。有时候我们不想让别人看到自己提交的代码，或者只是想让我们授权的人访问自己的代码，通常会有一下几种方式：\n成为付费用户（不限平台），创建私有仓库。 选择拥有一定数量免费私有仓库的平台，比如： Bitbucket 等。 自己搭建完全免费的 Git 服务，比如： Gitlab 等。 Gogs 属于最后一种方式，这是一个开源项目，核心代码使用 Go 语言编写， UI 使用的是 Semantic-UI。Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。 Go 目前支持 29 种语言，简体中文是必须的。\nGogs 的使用场景 Gogs 适合中小团队使用，工作流和大部分的 Git 服务类似，可以创建组织、仓库和工单（ issue），也可以把其他平台的仓库迁移过来。总的来说， Gog 能满足中小团队大部分的 Git 工作流需求。\n为什么推荐 Gogs 其实现在已经有很多类似的产品了，而且也都是免费的，有些甚至不用自己搭建，打开网页就能用，比如： Gitlab、码云等。我推荐 Gogs 只要是因为：\n完全开源，遵循 MIT 协议。 核心代码使用 Go 语言编写（个人喜好）。 官方完美支持中文。 极易搭建。 极简风格。 如何搭建 官方提供了 5 种搭建方式，分别是：\n二进制安装 源码安装 包管理安装 采用 Docker 部署 通过 Vagrant 安装 Gogs 需要将一部分数据保存到数据库中，所以需要数据库支持，可以使用以下几种数据库：\nMySQL：版本 \u0026gt;= 5.7 PostgreSQL MSSQL TiDB（实验性支持，使用 MySQL 协议连接） 或者 什么都不安装 直接使用 SQLite3 根据官方文档的安装步骤一步一步搭建即可。但是，偷懒是程序员的必备技能之一啊！其实，搭建就分两部分： Gogs 主程序和数据库。 Gogs 主程序官方提供了 Docker 镜像，数据库也能找到对应的 Docker 镜像，于是自然而然就能想到 Docker 三剑客之一的 Docker Compose。大家可以使用我写的一个 docker-compose.yml 文件（数据库使用的是 MySQL），地址是：https://github.com/blackmatch/gogs-docker-compose，几分钟就能轻松搭建好。\n一些截图 从官网搬来几张截图供大家欣赏：\n更多信息 官网：https://gogs.io/\n官方体验网站：https://try.gogs.io/\n","date":"2018-09-22T21:34:58+08:00","permalink":"https://www.blackmatch.cn/p/%E4%BD%BF%E7%94%A8-gogs-%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84-git-%E6%9C%8D%E5%8A%A1/","title":" 使用 Gogs 搭建自己的 Git 服务 "},{"content":"前言 今天测试 MM 给我提了个 issue：某个页面点击查询后就卡死了。由于查询的数据量比较多，我第一反应就是相关接口的 SQL 语句需要优化，但是我调试的时候发现几条查询语句都在 500 毫秒内得到结果返回了，而且这几条查询是并发执行的，所以 SQL 语句没问题，经过调试发现，主要耗时在程序的循环遍历，最外层的循环有 250 多次，每次遍历需要消耗 500 毫秒左右，这样的话接口返回就需要 2 分钟左右，而前端请求接口的超时默认是 80 秒，超时后走 catch 回调，估计前端也没有处理 catch ，所以导致界面一直转圈圈卡死了。言归正传，接口的主要耗时是在程序的循环操作，因为循环中有嵌套，总共 3 层（代码写得好烂，但是基于现在的业务逻辑，也只能这样了），通过 debug ，发现最耗时的操作在最里层的循环中。\n分析问题 在第 2 层的循环，有一个操作是：从一个数组中找出符合条件的所有元素，取这些元素的值进行一些计算，使用的方式也是循环遍历。我们把这个元素称为 arrayData，这个数组的长度为 15000 左右，遍历这个数组的操作便是第 3 层循环。第 1 层循环次数是 250 次左右，第二层循环次数是 4 次，第 3 层循环次数是 15000 次左右。第 3 层循环在第 2 层循环中会执行多次，代码结构是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 for () { // 第 1 层循环 for (){ // 第 2 层循环 arrayData.forEach((item) =\u0026gt; { // 第 3 层循环 }); arrayData.forEach((item) =\u0026gt; { // 第 3 层循环 }); ... } } 感觉有点不忍直视。。。自己写的代码，哭着也要调试完。第 3 层循环，完整的走完一遍，大概需要 80 毫秒左右，然后在第 2 层循环中有 4 次这样的循环，所以这部分操作就消耗了 320 毫秒左右。所以，我把注意力集中在如何优化这 320 毫秒上。\n优化方案 从业务逻辑优化 我首先考虑的是从业务逻辑上进行优化，经过排查代码，发现一个可行的方法是：减少 arrayData 的长度。arrayData 可以从业务逻辑上进行分组，得到几个长度大约 2000 左右的数据，这样第三层循环的时候就大大减少了循环次数，原来是 15000*4=60000 次，现在是 2000*4=8000 次，减少了 52000 次。经过这样优化，第 3 层循环每次大概需要 40 毫秒左右，原来的耗时是 80 毫秒，这样对最外层的循环来说，每次减少了 40*4=160 毫秒，总时间减少了 250*160 = 40000 毫秒，也就是 40 秒！我滴乖乖！虽然总时间减少了 40 秒，但还是很慢。\n从数组遍历方式上优化 从上面的代码结构中可以看到，第 3 层循环使用的是 forEach 来进行遍历的，第 1 层和第 2 层其实使用的是 ES6的 for-of 进行遍历的。这个差别让我产生了疑虑：这两种遍历方式会不会存在性能上的差异？于是，我把第 3 层的遍历方式从 forEach 换成了最古老的 for 循环遍历，代码是这样的：\n1 2 3 for (let i = 0, len = arrayData.length; i \u0026lt; len; i++) { const item = arrayData[i]; } 需要注意一个细节，在数组长度不会动态变化的情况下，要用一个变量来接数组的长度，不要直接写 i\u0026lt;arrayData.length，据说也会对性能有细微的影响。\n这样一改，奇迹出现了。总时间又减少了 20 秒左右。然后再东补补西补补，接口总耗时 30 秒左右，对我们的系统来说已经是可以接受的程度了，然后代码提交发布，打完收工，测试 MM 对我刮目相看。哈哈哈。。。等等，还没完呢！请继续往下看。\nJavaScript 中的数组 JavaScript 的数组与其他编程语言的数组有较大的差别，首先， JavaScript 的数组元素可以是任意类型，一个数组，第 1 个元素可以是数字，第 2 个元素可以是字符串，第 3 个元素可以是对象；其次， JavaScript 的数组其实也是对象，有对象就有 键 和 值，默认情况下， JavaScript 会给数组每一个元素创建一个从 0 开始的键，可以通过 [key] 访问数组中的元素，如果 key 传的是整型，会自动转换成字符串类型。举例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 const arr1 = [1, \u0026#39;str\u0026#39;, { name: \u0026#39;blackmatch\u0026#39; }]; console.log(arr1[1]); // str console.log(arr1[\u0026#39;1\u0026#39;]); // str console.log(arr1.length); // 3 console.log(Object.keys(arr1)); // [ \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39; ] const arr2 = [\u0026#39;str1\u0026#39;, \u0026#39;str2\u0026#39;, \u0026#39;str3\u0026#39;]; console.log(arr2[1]); // str2 console.log(arr2[\u0026#39;1\u0026#39;]); // str2 console.log(arr1.length); // 3 console.log(Object.keys(arr2)); // [ \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39; ] const arr3 = []; arr3[\u0026#39;name\u0026#39;] = \u0026#39;blackmatch\u0026#39;; arr3[\u0026#39;age\u0026#39;] = 18; console.log(arr3); // [ name: \u0026#39;blackmatch\u0026#39;, age: 18 ] console.log(arr3[\u0026#39;age\u0026#39;]); // 18 console.log(arr3.age); // 18 console.log(arr3.length); // 0 console.log(Object.keys(arr3)); // [ \u0026#39;name\u0026#39;, \u0026#39;age\u0026#39; ] const arr4 = []; console.log(arr4.length); // 0 arr4[\u0026#39;0\u0026#39;] = 1; console.log(arr4.length); // 1 console.log(arr4); // [ 1 ] const obj = { name: \u0026#39;blackmatch\u0026#39;, age: 18 }; console.log(obj.length); // undefined console.log(Object.keys(obj)); // [ \u0026#39;name\u0026#39;, \u0026#39;age\u0026#39; ] 这里比较有意思的是，arr3.length 输出的结果是 0 。其实原因很简单：因为 arr3 中没有元素。通过 arr3['age']=18 的方式并不能给 arr3 添加元素，其原因是：\n在 ECMAScript 中，Object 是所有对象的基础，因此所有对象都具有 Object 的基本属性和方法。\n也就是说， arr3['age']=18 只是给 arr3 添加了一个名为 age 的 键，并将其 值 设置为 18 ，并没有往 arr3 中添加元素，而 arr3.length 返回的是 arr3 元素的个数。给数组添加元素可以通过 对象字面量、push、unshift 等方法实现，也可以通过 arr[key]=value 的形式来添加或者修改元素，需要注意的是这里的 key 必须是字符串的数字，比如： arr['1']='element' ，由于数组的最大长度是 (2 ^ 32) - 1 ，所以数组元素的 key 的范围是 0 ~ (2 ^ 32) - 2 。 JavaScript 的 Array 对象还有很多细节值得关注，这里就不一一展开了。\nJavaScript 中数组的遍历方式 JavaScript 数据遍历主要有以下 3 种方式：\n最古老的 for 循环遍历 1 2 3 for (let i = 0, len = rows.length; i \u0026lt; len; i++) { const row = rows[i]; } forEach 遍历 1 2 3 row.forEach((row, idx, arr) =\u0026gt; { }); ES6 的 for-of 遍历 1 2 3 for (const row of rows) { } 这 3 种遍历方式各有千秋，具体该使用哪种方式需要结合具体的业务场景，但是这 3 种遍历方式在性能上是存在差异的，我在 jsPerf 平台对这 3 种方式进行了性能比较。测试方式很简单：先准备一个长度为 20000 的数组，数组的每个元素是一个对象，然后分别使用这 3 种方式对数组进行遍历，对每个元素做一个简单的赋值操作，最后比较每种方式的耗时。准备代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;script\u0026gt; var rows = []; for (let i = 0; i \u0026lt; 20000; i++) { const row = { name: \u0026#39;blackmatch\u0026#39;, age: 18 }; rows.push(row); } function handleRow(row) { if (row \u0026amp;\u0026amp; row.age \u0026gt; 0) { row.age = row.age * 2; } } \u0026lt;/script\u0026gt; 测试结果如下：\n可以看出，在性能方面：for\u0026gt;for-of\u0026gt;forEach。测试地址为：https://jsperf.com/for-vs-foreach-vs-for-of-by-blackmatch/1 。所以，如果需要遍历的数组长度比较大的时候，需要考虑一下性能问题。另外，不推荐使用 for-in 对数组进行遍历，具体原因就不赘述了。\n后话 大家可能觉得一个接口要耗时几十秒，这能忍？说实话，我也不能忍，我的优化方式可能也不对；但是从结果来看，从原来的一直卡死，到现在的几十秒能出结果，至少这个功能现在能用了。还有最关键的一点就是，我们的客户甚至我的领导，对性能真的一点都不在乎，能用就行了，他们更在乎的是增加需求，修改需求，领导也不希望我在性能调优上花太多时间，其实我做的这次优化更多的是从业务逻辑上优化了，因为我一开始就觉得代码逻辑以及计算等操作不会消耗太多时间，应该优化的是 SQL 语句和数据库结构，但事实上，性能优化涉及到每一个细节，只是在不同的环境下某些细节会更加突出，某些细节可以忽略不计。不说了，我要去改需求了。。。\n参考资料 《 JavaScript 高级程序设计（第 3 版）》\nhttps://zhuanlan.zhihu.com/p/23812134\nhttps://juejin.im/post/5a3a59e7518825698e72376b\n","date":"2018-09-20T21:17:52+08:00","permalink":"https://www.blackmatch.cn/p/javascript-%E4%B8%AD%E6%95%B0%E7%BB%84%E9%81%8D%E5%8E%86%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","title":"JavaScript 中数组遍历的那些事 "},{"content":"前言 现在的很多软件可能都会有省市县三级联动菜单，网上也有很多现成的数据。但是，我们在使用的时候往往会有一些顾虑：这是不是标准的数据？这数据是不是最新的啊？这数据不是我想要的格式。。。在进行开发的过程中有时候也需要造一些省市县相关的数据，大家第一反应可能会是 mock，但是我找了一下没有发现相关的模块，但是找到了一个 mockjs，于是我就依葫芦画瓢写了一个 mockz 模块。\n科普 我国的行政区域划分其实是分为 5 个部分的，分别是省、市、县、镇、乡。通常软件设计的时候，只会让用户选择省、市、县三级，也就是我们常说的省市县三级联动菜单。制定这些标准的是 国家统计局，可以在 这里 查看我国最新的行政区域划分标准。\n方案设计 基于上面的说明，其实要做的就只有两件事：\n从国家统计局获取最新的行政区域划分标准数据。 提供 mock 接口。 获取数据 通过爬虫获取最新的数据，这里不再赘述。爬取到数据后，将数据保存成 .json 文件，方便后续引用。数据的格式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ { \u0026#34;name\u0026#34;: \u0026#34; 北京市 \u0026#34;, children: [ { \u0026#34;code\u0026#34;: \u0026#34;110100000000\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34; 市辖区 \u0026#34;, children: [ { \u0026#34;code\u0026#34;: \u0026#34;110101000000\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34; 东城区 \u0026#34; } ] } ] } ] 爬虫相关的代码暂时不公布，写得不好。使用爬虫的好处是：可以随时获取最新的数据，可以定制自己喜欢的数据格式。\n提供 API 接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 const mockz = require(\u0026#39;mockz\u0026#39;); // 全部数据 const all = mockz.all(); console.log(all[0]); // 随机获取一个省级单位 const province = mockz.province(); console.log(province); // 山西省 // 随机获取一个市级单位 const city = mockz.city(); console.log(city); // 咸阳市 // 随机获取某个省的一个市级单位 const city2 = mockz.city(\u0026#39; 广东省 \u0026#39;); console.log(city2); // 潮州市 // 随机获取一个县级单位 const county = mockz.county(); console.log(county); // 怀柔区 // 随机获取某个省的一个县级单位 const county2 = mockz.county(\u0026#39; 广东省 \u0026#39;); console.log(county2); // 蕉岭县 // 随机获取某个省某个市的一个县级单位 const county3 = mockz.county(\u0026#39; 广东省 \u0026#39;, \u0026#39; 广州市 \u0026#39;); console.log(county3); // 天河区 // 随机获取一个地址 const address = mockz.address(); console.log(address); // 山西省晋中市昔阳县 mockz.all() 可用来做省市县三级联动菜单的数据源。\n最后 每一件小事都值得用心思考，然后付诸行动，不求一鸣惊人，只愿每日精进。模块的源码地址如下：\nhttps://github.com/blackmatch/mockz\n","date":"2018-09-09T21:15:48+08:00","permalink":"https://www.blackmatch.cn/p/%E5%86%99%E4%BA%86%E4%B8%AA%E7%9C%81%E5%B8%82%E5%8E%BF%E7%9A%84-mock-%E6%A8%A1%E5%9D%97/","title":" 写了个省市县的 mock 模块 "},{"content":"应用场景 有 500 多个 .txt 文件，每个文件几十 M，需要把这些文件的内容全部写入到数据库中，文件中每一行是一条数据，每个文件大约 200 万条数据。\n处理流程 这也是数据流动的过程，期望达到的效果是：快速、高性能（占用内存小）。这 3 个步骤中，解析、处理文件内容 这个步骤基本不会存在什么问题。问题的难点在 读文件 和 写入数据库 这两个步骤。\n读文件 一看到读取大文件或者批量文件，可能大家第一反应就是 stream。我也是这样想的，但是这里不能直接使用可读流来读取文件，前面提到，每个文件中每一行是一条数据，但是通过可读流读取文件的时候是以 buffer 的形式（也可以设置其他形式，比如 objectMode）读取的，并不是按照一行一行的方式读取，或许通过一些技巧可以实现，但是没有必要，因为有现成的 readline 模块。readline 模块读取文件的思想和可读流是一样的，只不过该模块每一次读取的数据是文件中的一行而已，通过 readline 模块可以实现逐行读取文件，且不会造成内存压力。\n写入数据 我使用的是 MySQL 数据库，因为文件中会有很多重复的内容，所以我给表的某个字段添加了 UNIQUE 索引，为了后期使用方便，我给 2 个字段添加了 INDEX 索引。这种做法对提高数据质量和查询速度有一定的帮助，但相对的，会对数据写入造成一定的影响，简单点说就是写入速度会变慢。本文不展开讨论如何提升数据库的写入速度，只是交代有这么个情况而已。写入数据库通过 knex 和 mysql 这两个模块来实现。\n读取和写入速度差异的问题 我们姑且忽略中间解析、处理文件内容所消耗的时间，把注意力集中在数据的读取和写入上。上面提到，写入数据库的过程是比较慢的，此外每次有数据需要写入的时候都得先连接数据库，等待数据库反馈，然后再写入下一条数据。由于读取数据的速度是非常快的，一个文件 2 秒左右就读取完成了，但是要把一个文件的所有数据完全写入到数据库中需要花费 30 分钟左右，这种巨大的速度差异最终会给内存带来很大的压力，没有写入到数据库的数据一直在内存中堆积，直到最终内存爆了，程序异常退出。\n解决读写速度差异问题 其实这个问题概括一下就是：读太快了，写太慢了。解决思路也就有 2 条了： 1. 读慢一点； 2. 写快一点。最理想的情况是读写速度趋向于平衡，且读写速度都非常快，但是要达到这种状态，以我目前的能力是不可能的（求大佬带）。所以我选择了第一种方案：降低读取速度。经过测试，完全读取一个文件的所有内容到内存中等待写入不会对我的机器造成内存压力，占用大概 100 多 M 内存。所以，我最终的解决方案是：一次读取一个文件的数据，把这些数据全部扔给可写流，可写流把这些数据一条一条写入到数据库中，等可写流把队列中所有的数据都写入到数据库后，再读取下一个文件的内容，然后把数据继续扔到可写流中，如此循环操作，直到所有的文件都处理完成。这样做会有 2 个好处：1.不会造成内存压力；2.可写流中一直有数据在等待写入数据库，不会造成资源浪费。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 const dealFile = (file, writer) =\u0026gt; { const dst = path.join(\u0026#39;mail\u0026#39;, file); const rl = readline.createInterface({ input: fs.createReadStream(dst), }); rl.on(\u0026#39;line\u0026#39;, (line) =\u0026gt; { writer.write(line); }); rl.on(\u0026#39;close\u0026#39;, () =\u0026gt; { debug(`文件 ${file} 读取完毕`); }); }; class WStream extends Stream.Writable { constructor(opts) { super(); this.counter = 0; this.txtFiles = opts.txtFiles; this.fileIdx = 0; this.on(\u0026#39;nextFile\u0026#39;, (file) =\u0026gt; { dealFile(file, this); }); } start() { this.emit(\u0026#39;nextFile\u0026#39;, this.txtFiles[this.fileIdx]); } _write(chunk, encoding, next) { const rx = new RegExp(/\\S+[a-z0-9]@[a-z0-9\\.]+/img); const mArr = chunk.toString().trim().match(rx); if (mArr \u0026amp;\u0026amp; mArr.length \u0026gt; 0) { const email = mArr[0].trim(); const password = chunk.toString().trim().replace(rx, \u0026#39;\u0026#39;).replace(\u0026#39;----\u0026#39;, \u0026#39;\u0026#39;).trim(); if (email.length \u0026gt; 0 \u0026amp;\u0026amp; password.length \u0026gt; 0) { knex(\u0026#39;netease\u0026#39;).insert({ email, password, }).then((result) =\u0026gt; { this.counter += 1; if (this.counter % 10000 === 0) { debug(`队列中还有 ${this.writableLength} 数据需要处理`); this.counter = 0; } next(); if (this.writableLength === 0) { this.fileIdx += 1; if (this.fileIdx \u0026lt; this.txtFiles.length) { this.emit(\u0026#39;nextFile\u0026#39;, this.txtFiles[this.fileIdx]); } else { debug(\u0026#39;all done!\u0026#39;); process.exit(0); } } }).catch((err) =\u0026gt; { this.counter += 1; if (this.counter % 10000 === 0) { debug(`队列中还有 ${this.writableLength} 数据需要处理`); this.counter = 0; } next(); if (this.writableLength === 0) { this.fileIdx += 1; if (this.fileIdx \u0026lt; this.txtFiles.length) { this.emit(\u0026#39;nextFile\u0026#39;, this.txtFiles[this.fileIdx]); } else { debug(\u0026#39;all done!\u0026#39;); process.exit(0); } } }); } else { next(); } } else { next(); } } } const run = () =\u0026gt; { const files = fs.readdirSync(\u0026#39;./mail\u0026#39;); const txtFiles = _.filter(files, (file) =\u0026gt; { return file.endsWith(\u0026#39;.txt\u0026#39;); }); debug(`共有 ${txtFiles.length} 个文件`); const writer = new WStream({ txtFiles, }); writer.start(); }; run(); 上述是核心代码，写得有点烂，还有很多可以优化的空间，这里我们只看思路就好了。几个关键点：\n写一个可写流的类，继承 Stream.Writable，并实现 _write 方法。 每读取文件中的一行数据，就调用可写流的 .write 方法，把这条数据扔到可写流中等待处理。 判断可写流中有没有数据需要处理，通过 writable.writableLength 属性来判断，这个属性是在 Node.js 的 v9.4.0 版本中加入的。需要注意的是，这个属性在这里指的是字节数，不是行数。 需要在调用 next() 方法之后再去判断 writable.writableLength 的值，否则会出现程序卡死，因为如果在 next() 方法之前判断的话，处理可写流队列中最后一条数据的时候 writable.writableLength 的值是大于 0 的，而这又是最后一次调用可写流的 .write 方法，所以不会继续读取下一个文件了，造成程序卡死。 总结一下 Node.js 的 stream 模块及其理念在原生模块中应用非常广泛，比如：HTTP.Request、HTTP.response、process.stdin、process.stdout 等。这其实类似于 大事化小小事化了 的思想，我一开始也尝试过一些其他的解决方案，比如：\n并发读取多个文件，通过 Promise 和 async/await 控制并发。 一个文件一个文件读取，读取完一个文件后等待 25 分钟再读取下一个文件。 这两种方案都不是很理想，没能解决 读写速度差异问题。由于 stream 用得也不多，所以走了一些弯路，但就结果而言，目前的解决方案能够比较优雅的解决了我的问题，同时也能学到了一些东西。文章肯定存在很多纰漏，欢迎各位大佬指正。\n","date":"2018-09-08T21:11:46+08:00","permalink":"https://www.blackmatch.cn/p/%E5%88%A9%E7%94%A8-stream-%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86%E6%96%87%E4%BB%B6/","title":" 利用 Stream 批量处理文件 "},{"content":"letsencrypt 提供了免费的 SSL 证书服务，我们可以用它来实现 HTTPS。我的服务器系统是 CentOS7 ，使用的 server 程序是 nginx。下面分享我使用 letsencrypt 的过程。官方推荐使用 certbot 工具。\n开启 CentOS 的 EPEL 扩展 1 wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 1 yum install -y epel-release-latest-7.noarch.rpm 安装 certbot 1 yum install -y certbot-nginx 停止 nginx 服务 1 systemctl stop nginx 生成 SSL 证书 1 certbot certonly --standalone -d blackmatch.cn -d www.blackmatch.cn 中途需要输入邮箱、同意协议等，按照提示操作即可。\n把域名替换成你自己的域名。\n配置 nginx 文件，编辑 /etc/nginx/conf.d/default.conf，参考我的配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 server { listen 80; server_name blackmatch.cn www.blackmatch.cn; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name blackmatch.cn www.blackmatch.cn; ssl_certificate /etc/letsencrypt/live/blackmatch.cn/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/blackmatch.cn/privkey.pem; ssl_session_cache shared:le_nginx_SSL:1m; ssl_session_timeout 1440m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers \u0026#34;ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS\u0026#34;; root /usr/share/nginx/html; index index.html index.htm; error_page 404 /404.html; } 保存退出文件\n启动 nginx 1 systemctl start nginx 测试 在浏览器访问 blackmatch.cn，如果配置成功应该像这样的：\n如果失败，请根据错误提示寻找合适的解决方案。\n定期更新证书 证书的默认有效期是 3 个月，所以每三个月要更新一次证书。可以通过 Linux 的 crontab 来定期更新证书，使用 crontab -e 命令打开编辑界面，最后添加一行：\n1 0 0 1 * * certbot renew 然后保存并退出。这样就设置了每个月 1 号的 0 点 0 分自动刷新证书。\n","date":"2018-05-08T21:07:51+08:00","permalink":"https://www.blackmatch.cn/p/centos-%E9%80%9A%E8%BF%87-letsencrypt-%E5%AE%9E%E7%8E%B0-https/","title":"CentOS 通过 letsencrypt 实现 HTTPS"},{"content":"IBM DB2 是美国 IBM 公司开发的一套关系型数据库管理系统，它主要的运行环境为 UNIX（包括 IBM 自家的 AIX）、 Linux、 IBM i（旧称 OS/400 ）、 z/OS，以及 Windows 服务器版本。 DB2 主要应用于大型应用系统，具有较好的可伸缩性，可支持从大型机到单用户环境，应用于所有常见的服务器操作系统平台下。\n我需要实现的技术方案如下：\n需要从 db2 数据库中取数，然后把数据写入到 MySQL 数据中。\n理论知识 DB2 数据库几个概念\ninstance, 同一台机器上可以安装多个 DB2 instance。\ndatabase, 同一个 instance 下面可以创建有多个 database。\nschema, 同一个 database 下面可以配置多个 schema。 所有的数据库对象包括 table、 view、 sequence， etc 都必须属于某一个 schema。\n另外， database 是一个 connection 的目标对象，也就是说用户发起一个 DB2 连接时，指的是连接到到一个 database，而不是连接到一个 instance，也不是连接到一个 schema。\n但是 DB2 的启动和关停是以 instance 为单位的。可以启动一个 instance，或者关停一个 instance。但不可以启动或者关停一个数据库或者一个 schema。\n使用的模块 使用了 ibm_db，该模块在安装时会根据当前平台自动下载对应的客户端驱动程序。\n第一坑 遇到的第一坑是 CODEPAGE（代码页），可以简单的理解为这是数据库的编码，在 db2 数据库数据库中，如果客户端和服务端的 CODEPAGE 不一致，连接时会报错：\n1 SQL0332N Character conversion from the source code page \u0026#34;1386\u0026#34; to the target code page \u0026#34;819\u0026#34; is not supported 而使用的 ibm-db 中没有提到如何设置 CODEPAGE 的方式，在各种 google 攻略后，得到的解决方案有两个：\n修改客户端的操作系统语言。\n添加系统环境变量 DB2CODEPAGE。\n我的后端架构为：\n最外层是 ubuntu 系统，然后起一个容器，容器是基于 ubuntu16.04 的，然后在容器中有一个 ETL 模块，这是一个 node 模块， node 通过调用这个模块去连接 db2 数据库。\n不论我怎么修改最外层的 ubuntun 系统还是容器中的 unbuntu 系统的语言和环境变量，都不起作用。\n最终解决方案：\n在 ETL module 模块中，在连接 db2 代码之前设置环境变量：\n1 process.env.DB2CODEPAGE = 1386; 第二坑 成功连接到 db2 数据库后，发现取到的数据的中文是乱码。于是又开始在网上找攻略，大多数答案都是说 CODEPAGE 问题。可是上一个坑已经解决了呀。优于无法直接访问到 db2 所在的服务器，所以无法很准确的确认 db2 数据库使用的 CODEPAGE 值，但是经过各种调试及执行如下 SQL 语句：\n1 SELECT CODEPAGE FROM SYSCAT.DATATYPES WHERE TYPENAME = \u0026#39;VARCHAR\u0026#39;; 得出的结论都是： db2 数据库的 CODEPAGE 是 1386 ，可以理解成 db2 数据库的编码是 GBK。所以我把客户端的 CODEPAGE 设置成 1386 应该是没有问题的呀？但是实际情况就是中文无法正确展示。\n最终解决方案：\n1 process.env.DB2CODEPAGE = 1208; 将客户端的 CODEPAGE 设置成 1208 即可。一脸懵逼啊！此方案是我拍脑袋尝试后得出的。 1208 对应的编码是 utf8 。\n其他坑 环境问题，项目自身的打包发布流程存在各种坑。\nibm_db 问题， 2.1.0 之前的版本在连接上是存在一些问题的，我开始折腾的时候是 2.3.0 版本，其实这个版本也有一些问题，然后我提了个 issue，过了两天后更新到了 2.3.1 ，使用这个版本后神奇的解决了连接问题。\n使用 ibm_db 去连接 db2 数据库，不是真正的命令行客户端连接，而是使用了一个驱动程序去连接，所以网上的在客户端执行 db2set DB2CODEPAGE=1386 的方法都行不通。\n使用 Node.js 连接 db2 数据库的相关资料较少，使用的人也少，搜到很多都是 java 的资料。\n总结 为了实现开头提到的技术方案，我加班加点花了差不多一个礼拜的时间，除了项目本身的打包、运行环境的坑以及对 db2 不熟悉外，其他问题大概花了两天左右。经过一个礼拜的折腾，对解决问题有了一些心得：\n环境很重要，因为我在本地开发环境是执行代码是比较顺利的，但是现场环境比较复杂，所以在解决问题之前要充分了解现场的软件环境，包括操作系统、版本等。\n如果是没遇到的东西，最后先去了解基本的概念、必要的基础知识。\n从最终代码执行处入手，如果我一开始就在使用代码连接 db2 的地方通过 process.env.DB2CODEPAGE 打印出来的话，可能会省很多时间。\n参考资料 https://www.jianshu.com/p/e1f38505f789\n","date":"2018-04-24T02:27:07+08:00","permalink":"https://www.blackmatch.cn/p/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8-db2-%E6%95%B0%E6%8D%AE%E5%BA%93%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","title":" 记一次使用 DB2 数据库遇到的坑 "}]